# Декорреляция. Метод Xavier. Метод He

## Декорреляция
Это процесс уменьшения корреляции между признаками данных. Цель декорреляции - сделать признаки как можно более независимыми друг от друга.
#### Зачем?
- **Улучшение обучения моделей**: модели машинного обучения, такие как линейные регрессии и нейронные сети, могут лучше обучиться, если признаки независимы друг от друга
- **Снижение избыточности**: уменьшение корреляции между признаками снижает избыточность в данных
- **Повышение устойчивости**: модели становятся более устойчивыми к переобучению
**Примеры методов декорреляции**:
- **Метод главных компонент (PCA)**: преобразует исходные данные в набор новых, некоррелированных признаков (главных компонент), главные компоненты ранжируются по убыванию их дисперсии, и наиболее значимые компоненты могут быть использованы для уменьшения размерности данных
- **Факторный анализ (Factor Analysis)**: выделяет скрытые факторы, которые объясняют наблюдаемые корреляции между признаками, используется для уменьшения размерности данных и устранения корреляций
- **Whitening (PCA/ZCA Whitening)**: преобразование данных таким образом, что признаки становятся некоррелированными и имеют одинаковую дисперсию

## Методы инициализации весов Xavier и He

#### Зачем нужно инициализировать веса?
В начале обучения нейронной сети веса задаются случайным образом. Если выбрать их неправильно, сеть может плохо обучаться. Правильная инициализация весов помогает нейронной сети быстрее и лучше находить оптимальные значения весов.

#### Проблемы с плохой инициализацией весов:
1. **Затухающие градиенты (vanishing gradients)**:
	Если начальные веса слишком малы, сигналы и градиенты становятся очень маленькими, когда проходят через слои сети. В результате, изменения весов становятся настолько малыми, что обучение практически останавливается.
1. **Взрывающиеся градиенты (exploding gradients)**:
    Если начальные веса слишком велики, сигналы и градиенты становятся очень большими. Веса изменяются слишком сильно, что делает обучение нестабильным.

### Метод Xavier
Помогает избежать затухающих и взрывающихся градиентов, правильно выбирая начальные веса.
#### Как работает метод Xavier?
Метод Xavier выбирает веса так, чтобы сигналы (активации) и градиенты имели одинаковую дисперсию (вариативность) на всех слоях сети.

### Метод He
Делает то же самое, но особенно хорошо работает с функцией активации ReLU
#### Как работает He?
Метод He выбирает веса так, чтобы сигналы и градиенты имели одинаковую дисперсия на всех слоях сети, учитывая особенности ReLU. ReLU пропускает только положительные значения и обнуляет отрицательные. Метод He учитывает это и инициализирует веса немного иначе.
#### Почему He лучше подходит для ReLU?
Когда используем ReLU, половина нейронов на каждом слое обычно "умирает" (их выход равен 0). Метод He учитывает это и задает веса так, чтобы оставшаяся половина нейронов давала подходящие значения для продолжения обучения. Это помогает избежать затухающих градиентов, что часто происходит при использовании ReLU с другими методами инициализации.
