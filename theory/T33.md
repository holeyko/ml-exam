# Уменьшение размерности. Синтез признаков. Выбор признаков. Алгоритмы фильтрации

В задаче **уменьшение размерности** у нас имеется объекты, которые описаны признаками $\mathcal{F} = (f_1, \ldots, f_n)$, по которым мы хотим построить множество признаков $\mathcal{G} = (g_1, \ldots, g_k) : k < n$ (часто новое множество сильно меньше по мощности), переход к которым сопровождается наименьшей потерей информации. В данной задаче, помимо несильного уменьшения точности (допускается небольшое отклонение), мы добиваемся:

- Ускорение обучения и обработки
- Борьба с шумом и мультиколлинеарностью
- Интерпретация и визуализация данных (например, мы смогли свести данные к двумерному или трехмерному случаю)

Есть два основных метода уменьшения размерности: *выбор признаков* и *синтез признаков*. Чем же они отличаются?

- **Выбор признаков** включает признаки $\mathcal{G}$ такие, что $\mathcal{G} \subset \mathcal{F}$. Грубо говоря, признаки выбираются только из $\mathcal{F}$.
- **Извлечение признаков** включает признаки $\mathcal{G}'$ такие, что $\mathcal{G} \subset \mathcal{F}$ и $\exists x \in \mathcal{G}: x \ni \mathcal{F}$. Грубо говоря, признаки могут выбираться из $\mathcal{F}$ и могут быть придуманы алгоритмом.

**Фильтрующие методы** пытаются оценить каждый признак какой-то мерой значимости и после этого отсортировать по данной значимости и выбрать наиболее значимые.

![Фильтр](assets/filter.png)

Классификация фильтрующих методов: бывают *одномерные* и *многомерные*. Стоит отметить, что эти методы существуют только для отдельных рассматриваемых признаков и случаев задачи.

- Если у нас числовой параметр и наша задача - регрессия, то нам нужна мера, которая сравнит вектора чисел. Сюда подойдут Евклидово расстояние и, например, коэффициент корреляции (Пирсона или Спирмена).
- Есть меры, которые вычисляют взаимосвязь числового и категориального признаков. Сюда подойдут попарные расстояния (внутренние или внешние) и условная дисперсия.
- Есть меры, которые оценивают две категории: прирост информации (IGain), индекс Джини и $\chi^2$.

К многомерным относится более сложные и медленные, например, *транспонирование + кластеризация*.

Пример с корреляцией. Коэффициент корреляции Пирсона, наше значение лежит в $[-1, 1]$:

$$
  r = \dfrac{\sum_{ij}{\left((x_{ij} - \hat{x_j}) \cdot (y_i - \hat{y})\right)}}{\sqrt{\sum_{ij}{\left((x_{ij} - \hat{x_j})^2\right)} \cdot \sum_{i}{(y_i - \hat{y})^2}}}
$$

В случае же с корреляцией Спирмана мы

1. Сортируем объекты двумя способами (по каждому из признаков).
2. Находим ранги объектов для каждой сортировки.
3. Вычисляем корреляцию Пирсона между векторами рангов.
