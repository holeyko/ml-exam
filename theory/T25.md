# Дропаут. Пакетная нормализация

## Дропаут

Это метод регуляризации, используемый для предотвращения переобучения в нейронных сетях. Он заключается в случайном "отключении" некоторых нейронов на каждом шаге обучения
**Зачем?**:

- **Снижение переобучения**: дропаут помогает избежать зависимости модели от определенных нейронов, что улучшает её обобщающую способность;
- **Улучшение устойчивости**: модель становится менее чувствительной к отдельным входным данным и более устойчивой к шуму
**Как работает?**:
- **На этапе обучения**: на каждом шаге обучения случайно отключается определенный процент нейронов (от 20% до 50%), эти нейроны не участвуют в передачи данных и обновлении весов на этом шаге, в результате модель вынуждена учиться на разных подмножествах нейронов, что улучшает обобщающую способность
- **На этапе предсказания**: используются все нейроны, но их выходные значения уменьшаются на коэффициент, равный вероятности их включения во время обучения, чтобы компенсировать увеличение количества активных нейронов (в процессе обучения их меньше, чем в предсказании)

## Пакетная нормализация

Это техника, используемая для ускорения и стабилизации обучения глубоких нейронных сетей.
**Зачем?**

- Ускорение обучения: позволяет использовать более высокие скорости обучения
- Стабильность обучения: нормализация входов слоев делает процесс обучения более устойчивым и предотвращает затухание и взрыв градиентов
- Улучшение обобщающей способности: батчевая нормализация действует как регуляризатор, снижая переобучение
**Как работает?**:
- **Представим себе обычный слой нейронной сети**:
  - **Входные данные**: мы подаем на вход какие-то данные (изображение, текст или что-то ещё)
  - **Преобразование входных данных**: для каждого нейрона вычисляется, например,  взвешенная сумма входных данных и прибавляется смещение $b$
  - **Функция активации**: например, ReLU изменяет преобразованные входные данные, чтобы получить выходные данные
- **Проблемы, которые решает батчевая нормализация**:
  - Без нормализации активации нейронов (выходы после активации) могут иметь разные распределения на каждом шаге обучения, что делает процесс обучения менее стабильным и медленным. Батчевая нормализация помогает стабилизировать эти активации
- **Шаги батчевой нормализации**:
  - **Выбор мини-батча**: весь обучающий набор данных делится на небольшие подвыборки, называемые мини-батчами, размером, например, 32. Если обучающий набор не делится нацело, то последний мини-батч может быть меньше.
  - **Вычисление среднего значения и дисперсии**: для каждого нейрона в слое вычисляется среднее значение и дисперсия его входов в мини-батче:
    $$\mu_B = \frac{1}{m}\sum_{i = 0}^m{x_i}$$
    дисперсия:
    $$\sigma^2_B = \frac{1}{m}\sum_{i = 1}^m{(x_i - \mu_B)^2}$$
  - **Нормализация входов**: для каждого входного значения $x_i$ мы вычисляем нормализованное значение:
    $$\hat{x} = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$$
  - **Масштабирование и смещение**: После нормализации вводятся дополнительные параметры $\gamma$  и $\beta$, которые обучаются и позволяют модели адаптироваться к данным:

Коротко говоря, на каждом шаге обучения мы считаем для мини-батча среднее значение и метрики. Затем для каждого входа для каждого нейрона для каждого слоя нормализовать его по указанным формулам.
