# Наивный байесовский классификатор

Рассмотрим задачу *бинарной классификации*, то есть выходами у нас будут $y \in Y = \{-1,\ +1\}$, здесь мы работаем с вероятностными признаками, а значит мы относим $x$ к тому классу $y$, у которого будет выше вероятность.

Фактически, мы решаем задачу: $P(y\ |\ x) = ?$, где $P$ - это вероятность. По теореме Байеса мы получаем следующее:

$$
  P(y\ |\ x) = \dfrac{P(y) \cdot p(x\ |\ y)}{p(x)},
$$

где $p$ - это плотность распределения вероятности. Здесь: $P(y)$ - априорная вероятность появления класса $y$, $p(x\ |\ y)$ - распределение образов $x$ для определенного класса $y$. Предположим, что нам известны все величины, тогда правилом выбора класса будет служить: мы выбираем тот класс, вероятность которого окажется больше.

$$
  a(x) = \argmax_{y \in Y}{P(y\ |\ x)} = \argmax_{y \in Y}{P(y) \cdot p(x\ |\ y)}
$$

В чём задача байесовской классификации? По сути, нам нужно как-то *оценить* условные плотности распределения вероятности $p(x\ |\ y)$ оптимально.

**Наивный байесовский классификатор** - простейший вероятностный классификатор, основанный на применении [теоремы Байеса](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0) со строгими (наивными) предположениями о независимости для вычисления [*апостериорной вероятности*](https://ru.wikipedia.org/wiki/%D0%90%D0%BF%D0%BE%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%B2%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C) $P(A\ |\ B)$.

Пусть все признаки у образов $x$ независимы в вероятностном смысле между собой $x = \left[\xi_1,\,\xi_2,\,\ldots,\,\xi_{n}\right]^{\mathrm{T}}$, тогда многомерная плотность распределения для них будет записано как произведение соответствующих одномерных, то есть:

$$
  p(x\ |\ y) = p(x_1\ |\ y) \cdot p(x_2\ |\ y) \cdot \ldots \cdot p(x_{n}\ |\ y) = \prod_{i = 1}^{n}{\left(p(\xi_i\ |\ y)\right)}
$$

Следует отметить, что наивность больше относится к процессу восстановления плотности, а не к классификации.
