# Метод линейной регрессии. Гребневая регрессия. Лассо Тибширани

## Линейная регрессия

### Модель многомерной линейной регресии

$$f(x, \theta) = \sum_{j=1}^m \theta_jf_j(x),\theta \in \mathbb{R^m}$$
У нас есть некоторая функция, которая зависит от $x$ и параметров $\theta$, мы делаем из этой фукнции просто функцию предсказания, то есть параметризуем нашу функцию как параметр. По хорошему нужно написать, что $f_\theta(x) = f(x, \theta)$.

Сама функция из себя представляет просто скалярно произведение. Мы каждый признак нашего вектора, умножаем на какой-то коеффицент и получаем результат, здесь удобнее уже записать сразу же в матричном виде, потому что скалярное произведение можно представить в виде произведения матриц. Если у нас есть множество объектов, то мы из этих объектов можем составить матрицу и использовать уже как матрицу. Можно соотвественно умножить матрицу на вектор и получить результат предсказания.

### Матричные обозначения

Здесь $f$ обозначает уже признак, возникает небольшая путаница

$$
F =\begin{pmatrix}
f_1(x_1) & \ldots & f_m(x_1)\\
\ldots & \ldots & \ldots\\
f_1(x_n)& \ldots& f_m(x_n)
\end{pmatrix}, \quad y=\begin{pmatrix}y_1 \\ \ldots \\ y_n\end{pmatrix}, \quad \theta=\begin{pmatrix}\theta_1 \\ \ldots \\
\theta_m\end{pmatrix}$$

### Эмперический риск в матричной записи

Это функция ошибки в матричном виде:
$$\mathcal{L}(\theta, \mathcal{D}) = \sum_{i=1}^{n} \left( f(x_i, \theta) - y_i \right)^2 = \|F\theta - y\|^2 \to \min_{\theta \in \mathbb{R}^m}$$
В качестве предсказания мы матрицу объектов $F$ умножаем на вектор коеффицентов $\theta$, дальше получаем вектор, и вычитаем из него другой вектор: $y$, целевой. Далее мы берем норму от получившегося значения (у нас получиться вектор), и также эту норму, можно представить как матричную операцию, можно взять вектор и множить его самого на себя. Соотвественно мы полунотью в матричном виде записали нашу задачу.  

### Система нормальных уравнений

Теперь получается, что если мы возьмем производную?
Если мы из функции ошибки возьмем производную, и приравняем ее к нулю, то мы получим некоторое аналитическое решение этой задачи:
Условие минимума:

$$\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = 2F^\top (F\theta - y) = 0$$

$$\theta^* = (F^\top F)^{-1} F^\top y$$

Оно достаточно похоже на решение, которое получается при решении обычных линейных уравнений. Если мы просто представим, что $F$ это квадратная матрица, которая просто умножается на $\theta$ и получается $y$ ($F\theta = y$), чтобы найти $\theta$ нам нужно сделать следующее: $\theta = F^{-1}y$. Соотвеетсвенно если мы подставим квадратную матрицу, такое уравнение мы и получим (мы просто описали частный случай). Такое аналитическое решение называется методом наименьших квадратов (так это иногда называют). По хорошему метод наименьших квадратов: когда мы просто говорим, что у нас функция -- сумма квадратов разностей. Также здесь еще вводиться несколько сущностей:
$F^{+} = (F^\top F)^{-1}F^\top$ -- псевдообратная матрица (обратное преобразование Мура-Пенроуза)

$P_F = FF^{+}$ -- проекционная матрица

Решение:
$$\theta^* = F^+y$$
Это по факту то же самое, делаем проекцию просто.

Минимальное приблежение:
$$\mathcal{L}(\theta^*) = ||P_Fy - y||^2$$
Так мы можем представить нашу функцию ошибки.

Также мы можем не приравнивать к нулю производную, а использовать ее дальше для градиентного спуска.

## Гребневая регрессия

Гребневая регрессия -- это конкретное применение гребневой регуляризации в контексте линейной регресии. В гребневой регресии добавляется гребневая регуляризация к линейной регресии, что помогает справляться с проблемой мультиколлинеарности и переобучения.

### Гребневая регуляризация

К линейной регресии добавим квадрат наших коеффицентов, умноженный на коеффицент регуляризации. Мы предполагаем, что сложность модели зависит от величины $\theta$, а численно сложность можно выразить как $||\theta||^2$.

Тогда, мы говорим, что $\tau$ -- коэффицент регуляризации, а тогда, эмпирический риск мы представляем следующим образом:
$$\mathcal{L}_{\tau}(\theta) = ||F\theta - y|| ^ 2 + \tau||\theta||^2$$
Каждый коеффицент $\theta$_j можно регуляризовать отдельно. Например, можно учитывать веса признаков $w_j : \tau_j = 1/w_j$
Условие минимума в таком случае:
$$\frac{\partial L(\theta)}{\partial \theta} = 2 F^\top (F \theta - y) + 2 (\tau I_m) \theta = 0$$
Решение:
$$\theta_{\tau}^* = (F^\top F + \tau I_m)^{-1} F^\top y$$
Решение все еще аналитическое. Также можно заметить, что если до этого, когда мы работали с матрицами в контексте линейной регрессии, нормализация матрицы нас не волновала. То здесь нормализация важна, так как у нас есть разные коеффиценты при разных признаках, получается, что если у признака более большая дисперсия, то для него должен быть более маленький коеффицент регуляризации. Этому признаку будет соотвествовать более большой коеффицент $\theta$ просто потому, что этот признак более большой. Получается, что если набор данных не нормализован, то признаки мы будем нормализовывать по разному, а нам бы хотелось нормазовывать их унифицированно. Поэтому набор данных нужно нормализовать.

Например, мы можем теперь брать разные коеффиценты, чтобы по разному регуляризовать разные признаки. Чтобы каждый признак учитывался с разным весом. Если какой-то признак более важный, то соотвествующий коеффицент перед ним будет регуляризироваться меньше. Это применяют редко, но в основном используется один коеффицент ругялризации для всех.

Обратим внимание, что $\theta$ не являются весами признаков, хотя их очень часто так называют. Но можно сказать, что они влияют на ответ с каким-то весом и притом они могут влиять и в отрицательную сторону, получается отрицательный вес. В общем случае это не веса, просто какие-то коеффиценты.

Здесь мы также можем ввести понятие веса объекта, если мы унифицируем нашу функцию эмперического риска в матричной записи.

Также есть гребневая регуляризация для градиентного спуска, ключевая гипотеза заключается в том, что $\theta$ скачет, что и вызывает переобучение
Основная идея заключается в том, что мы граничим норму $\theta$.
Добавим штраф регуляризации для нормы параметров:

$$\mathcal{L}_{\tau}(a_{\theta}, D) = \mathcal{L}(a_{\theta}, D) + \frac{\tau}{2} \|\theta\|^2 \rightarrow \min_{\theta}$$
$\tau$ -- коеффицент регуляризации, отражает баланс между качеством и обощаемостью.
Регуляризация может быть легко учтена в градиентном спуске:
$$\nabla \mathcal{L}_{\tau}(\theta) = \nabla \mathcal{L}(\theta) + \tau \theta$$
$$\theta_{k+1} = \theta_k (1 - \mu \tau) - \mu \nabla \mathcal{L}(\theta)$$

## Лассо Тибширани

Это другой вид регуляризации для градиентного спуска, так называемая строгая регуляризация. По другому называется LASSO или Лассо Тибширане. Сама регуляризация это лассо, а решение этой задачи предложил Тибширани.

Предположение: значение вектора $\theta$ имеют распределение Лапласа:

$$\begin{cases}
\mathcal{L}_{\tau}(\theta) = \|F \theta - y\|^2 \rightarrow \min_{\theta} \\
\sum_{i=1}^{n} |\theta_i| \leq \kappa
\end{cases}$$

LASSO(least absolute shrinkage and selection operator)

Результирующая задача оптимизации получается следующей:

Где $||\theta||_1$ -- $l_1$ норма: $||\theta||_1 = \Sigma|\theta_i|$
Хорошего аналитического решения не существует для этой задачи, однако хорошее вычислительное решение существует.

Обе эти задачи называются LASSO и они эквивалентны друг другу на самом деле. Если мы перепишем нашу систему уравнений, то мы сможем получить нашу задачу оптимизации.

LASSO она называется по тому, как она зануляет достаточно много коеффицентов регуляризации, это связано с тем, как у нас выглядит наше ограничение (сумма $\theta_i$ должны быть меньше какой-то константы). Функция ошибки в свою очередь может быть суммой квадратов, тогда она будет выглядеть в нашем пространстве, как парабола (в двумерном пространстве), если минимум находиться внутри ромба, то задача решена, если минимум находиться снаружи, то получиться, что нас будет интересовать решение, которое находиться где-то вне ромба, то есть к нему надо будет еще придти, придется рассматривать все более большие срезы в нашей параболоиде. То есть наш будет интересовать случай, когда эллипс коснется нашего ромбика, но если бы $\theta$ находился выше и мы бы коснулись ромбика в вершине, это бы означало, что один из коеффицентов точно бы занулился.

Регуляризация -- это некоторые ограничения, которые мы накладываем на $\theta$ которые
