# Метод батчевой нормализации

**Батчевой нормализация** - это преобразование, которое применяется на промежуточных векторах между другими преобразованиями, при котором по каждой из координат по каждому объекта батчем считается *среднее* и *дисперсия*, применяем *нормализацию* и линейное преобразование, в данном случае масштабирование (так как после нормализации мы могли потерять какую-нибудь линейную взаимосвязь).

$$
  \begin{aligned}
    \mathbb{E}[x_{d}] = \dfrac{1}{n} \sum_{i = 1}^{n}{x_{i}} ~ & ~ \text{среднее значение} \\
    \mathbb{D}[x_{d}] = \dfrac{1}{n} \sum_{i = 1}^{n}{(x_{i} - \mathbb{E}[x_{d}])^{2}} ~ & ~ \text{дисперсия} \\
    \hat{x}_{d} = \dfrac{x_d - \mathbb{E}[x_{d}]}{\sqrt{\mathbb{D}[x_{d}] + \varepsilon}} ~ & ~ \text{нормализация} \\
    \hat{y}_{d} = \gamma_{d} \cdot x_d + \beta_{d} ~ & ~ \text{масштабирование}
  \end{aligned}
$$

Используется в нейронных сетях для ускорения обучения и улучшения стабильности. Работает путем нормализации входящих данных для каждого слоя, что делает распределение активаций более нормальным (среднее значение около 0 и стандартное отклонение около 1) в каждом мини-батче.

Преимущества батчевой нормализации:

- Ускорение обучения.
- Меньшая зависимость от инициализации весов.
- Уменьшение градиентного исчезновения и взрыва.
- Регуляризация.
- Устойчивость к сдвигам распределения.
