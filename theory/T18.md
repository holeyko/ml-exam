# Метод стохастического и пакетного градиентного спуска

## Пакетный градиентный спуск

Пусть нам дано некоторое допустимое множество $X \subset \mathbb{R}^n$ и целевая функция $f : X \to \mathbb{R}$ и мы хотим решить задачу оптимизации, где критерием поиска может выступать минимум или максимум. С точки зрения гладкости $f$ мы берём классификацию методов **первого порядка**, то есть вычисления первых частных производных (к ним относится *градиентный спуск*).

**Градиентный спуск** решает задачу минимизации эмпирического риска:

- $\theta_{(0)}$ - некоторое начальное значение;
- $\theta_{(k + 1)} = \theta_{(k)} - \mu\nabla\mathcal{L}(\theta_{(k)})$ - здесь мы говорим, что наш новый вектор коэффициентов - это старый вектор коэффициентов минус градиент от функции ошибки в старой точке $\nabla\mathcal{L}(\theta_{(k)})$ помноженный на $\mu$ - шаг градиента (или: скорость сходимости). Функция градиента $\nabla$ показывает лишь направление наискорейшего убывания функции, то есть он показывает лишь направление - и дале по этому вектору мы шагаем с скоростью сходимостью.

## Стохастический градиентный спуск

Заметим, что в [классическом варианте](#пакетный-градиентный-спуск) мы берём сумму по всем объектам при вычислении градиента, то есть мы для каждого объекта находимо градиент и потом складываем вместе все полученные градиенты. В таком случае, у нас будет жуткое замедление. Для решения это проблемы можно вычислять не все объекты, а лишь часть. А самым частным случаем - вообще один объект - такой градиентный спуск называется **стохастическим**.

В качестве критерия остановки можно взять, когда значения $\mathcal{L}$ и/или $\theta$ почти не меняются.
