# Бустрап. Случайный лес. Стэкинг

## Бустрап

**Бустрап** - это метод статистического ресемплирования, который используется для оценки характеристик генеральной совокупности на основе выборки.

1. Случайно извлекаем с заменой подвыборку того же размера, что и исходная выборка.
2. Строим модель на основе подвыборки.
3. Повторяем шаги 1 и 2 многократно (например, 1000 раз).
4. Рассчитываем статистику (например, среднее или доверительный интервал) на основе результатов нескольких моделей.

## Случайный лес

**Случайный лес** - структура, объединяющая идею *бустрапа* и дерева принятия решения. Состоит из $N$ деревьев - поэтому и называется как *лес*. Пусть у нас есть набор данных $\mathcal{D} = \{x_i, y_i\}_{i = 1}^{|\mathcal{D}|}$ с $n$ признаками. Тогда,

1. Выбираем подмножество какого-то фиксированного размера с повторениями.
2. Дальше синтезируем дерево решений, то есть, фактически, строим деревья решений таким образом, что для каждого дерева выбиралось $\sqrt{n}$ случайных признаков. При этом, сами деревья могут быть самыми примитивными, подрезка не применяется.
3. Повторяем п.1 и п.2 много-много раз.

Как агрегировать?

- Голосование. Деревья возвращают классы. Среди них ищется чаще всего встречающийся класс.
- Деревья могут возвращать распределения вероятностей или весов классов, которые суммируются для каждого класса. И далее полученные суммы используются как финальное распределение весов классов.

## Стэкинг

Идея **стэкинга**: вместо того, чтобы комбинировать алгоритмы, будем использовать их прогнозы для обучения других моделей. Эту штуку можно обобщить на использование результатов классификации как новых характеристик объектов.

Грубо говоря, мы взяли данные $\mathcal{D}$ и применили к нему алгоритм $a_1$, получили какой-то $\hat{y_1}$. После этого, мы присоединили $\hat{y_1}$ к данным $\mathcal{D}$ (в техническом плане: как новый столбец) в качестве новой характеристики и на этом обучаем алгоритм $a_2$ - получили $\hat{y_2}$ и таким образом строим такой "pipeline" пока не достигнем хорошего качества.
