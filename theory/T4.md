# Аугментация данных. Дропаут. ReLU

## Аугментация
Это процесс искусственного увеличения объёма обучающих данных путем создание новых данных из существующих.
**Зачем?**:
- **Увеличение объёма данных**: помогает в ситуациях когда данных недостаточно для обучения мощных моделей;
- **Улучшение обобщающей способности**: модель становится более устойчивой к различным вариациям данных;
- **Снижение переобучения (overfitting)**: добавление вариаций помогает модели не запоминать обучающие данные, а лучше улавливать общие закономерности
**Примеры аугментации изображений**:
- **Повороты**: вращение изображения на определённый угол;
- **Сдвиги**: перемещение изображения по горизонтали или вертикали;
- **Масштабирование**: изменение размера изображения;
- **Шум**: добавления случайного шума на изображение
**Примеры аугментации текста**:
- **Перефразирование**: изменение структуры предложения, сохраняя его смысл;
- **Синонимизация**: замена слов синонимами;
- **Удлинение или сокращение текста**: добавление или удаление несущественных слов
**Примеры аугментации аудио**:
- **Изменение темпа**: изменение скорости воспроизведения аудио;
- **Шум**: добавление фона или белого шума

## Дропаут
Это метод регуляризации, используемый для предотвращения переобучения в нейронных сетях. Он заключается в случайном "отключении" некоторых нейронов на каждом шаге обучения
**Зачем?**:
- **Снижение переобучения**: дропаут помогает избежать зависимости модели от определенных нейронов, что улучшает её обобщающую способность;
- **Улучшение устойчивости**: модель становится менее чувствительной к отдельным входным данным и более устойчивой к шуму
**Как работает?**:
- **На этапе обучения**: на каждом шаге обучения случайно отключается определенный процент нейронов (от 20% до 50%), эти нейроны не участвуют в передачи данных и обновлении весов на этом шаге, в результате модель вынуждена учиться на разных подмножествах нейронов, что улучшает обобщающую способность
- **На этапе предсказания**: используются все нейроны, но их выходные значения уменьшаются на коэффициент, равный вероятности их включения во время обучения, чтобы компенсировать увеличение количества активных нейронов (в процессе обучения их меньше, чем в предсказании)

## ReLU
Это функция активации, широко используемая в нейронных сетях из-за своей простоты и эффективности.
Функция преобразует входное значение $z$ следующим образом: $$ReLU(z) = max(0,z)$$
**Преимущества ReLU**:
- **Простота вычислений**
- **Решение проблемы исчезающих градиентов**: в отличии от сигмоидных и $tanh$ функций активации, ReLU не страдает от проблемы исчезающих градиентов
- **Спарсити**: ReLU приводит к обнулению выходов для отрицательных значений, что делает сеть более разреженной и эффективной
**Недостатки ReLU**:
- **Мертвые нейроны**: иногда нейроны могут "умирать", если их выход всегда равен нулю. Это может произойти, если веса обновляются таким образом, что выходные значения всегда отрицательны. **Решением** является использование модификации Leaky ReLU, допускающей небольшие отрицательные значения. $$Leaky ReLU(z) = \begin{cases} z, & \mbox{если } z > 0 \\ \alpha z, & \mbox{если } z \leq 0 \end{cases}$$
 \*обычно  $\alpha = 0.01$ 
