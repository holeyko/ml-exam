# Бустрап. Случайный лес. Стэкинг

**Бутстрап** - случайный выбор с повторами объектов из выборки. *Бэггинг* - метод, при котором один и тот же алгоритм обучается на подмножествах размера с бутстрапом.

**Случайный лес** - структура, объединяющая идею *бэггинга* и дерева принятия решения. Состоит из $N$ деревьев - поэтому и называется как *лес*. Пусть у нас есть набор данных $\mathcal{D} = \{x_i, y_i\}_{i = 1}^{|\mathcal{D}|}$ с $n$ признаками. Тогда,

1. Выбираем подмножество какого-то фиксированного размера с повторениями.
2. Дальше синтезируем дерево решений, то есть, фактически, строим деревья решений таким образом, что для каждого дерева выбиралось $\sqrt{n}$ случайных признаков. При этом, сами деревья могут быть самыми примитивными, подрезка не применяется.
3. Повторяем п.1 и п.2 много-много раз.

Как агрегировать?

- Голосование. Деревья возвращают классы. Среди них ищется чаще всего встречающийся класс.
- Деревья могут возвращать распределения вероятностей или весов классов, которые суммируются для каждого класса. И далее полученные суммы используются как финальное распределение весов классов.

Идея **стэкинга**: вместо того, чтобы комбинировать алгоритмы, будем использовать их прогнозы для обучения других моделей. Эту штуку можно обобщить на использование результатов классификации как новых характеристик объектов.

Грубо говоря, мы взяли данные $\mathcal{D}$ и применили к нему алгоритм $a_1$, получили какой-то $\hat{y_1}$. После этого, мы присоединили $\hat{y_1}$ к данным $\mathcal{D}$ (в техническом плане: как новый столбец) в качестве новой характеристики и на этом обучаем алгоритм $a_2$ - получили $\hat{y_2}$ и таким образом строим такой "pipeline" пока не достигнем хорошего качества.
