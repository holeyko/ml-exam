# Многослойная нейронная сеть. Автоматическое дифференцирование

##### Многослойная нейронная сеть (MLP)

**Структура MLP**:
- **Входной слой** содержит нейроны, которые принимают входные данные, количество нейронов соответствует числу признаков во входных данных
- **Скрытые слои** - это один или несколько слоев нейронов, которые обрабатывают входные данные, каждый нейрон в скрытом слое получает на вход **выходы всех нейронов из предыдущего слоя**
- **Выходной слой** содержит нейроны, которые выдают результат работы сети, количество нейронов соответствует числу классов в задаче классификации или одному нейрону в. задаче регрессии

MLP обучается с использованием алгоритма обратного распространения ошибки (backpropagation), состоящего из следующих шагов:
- **Прямое распространение (forward pass)**: входные данные проходят через все соли сети
- **Вычисление ошибки**: на выходном слое вычисляется ошибки (например, с использованием кросс-энтропии для классификации или среднеквадратичной ошибки для регрессии)
- **Обратное распространение (backpropagation)**: ошибка распространяется назад через все слои сети, градиенты шибки вычисляются для каждого веса с использованием **правила цепочки (chain rule)**
- **Обновление весов**: веса обновляются с использованием алгоритма градиентного спуска или его модификаций, например, Adam

##### **Автоматическое дифференцирование**

Это метод, который позволяет автоматически вычислять производные (градиенты) функции. Этот метод особенно полезен в обучении нейронных сетей, так как позволяет эффективно вычислять градиенты для обновления весов.

**Виды автоматического дифференцирование**:
- **Прямое автоматическое дифференцирование (Forward Mode)**: производная вычисляется одновременно с самой функцией, эффективен для функций с **малым числом входов** и **большим числом выходов**
- **Обратное автоматическое дифференцирование (Reverse Mode)**: производная вычисляется с использованием обратного распространения через вычислительный граф функции, эффективен для функций с **большим числом входов** и **малым числом выходов** (как в нейронных сетях)

**Почему используем автоматическое дифференцирование?**
Рассмотрим другие способы:
- **Численное дифференцирование**: $$f'(x) = \frac{f(x + h) - f(x)}{h}$$ Проблемы: погрешность из-за выбора h, медленное выполнение
- **Символьное дифференцирование**: $$f(x) = x^2 \to f'(x) = 2x$$Проблемы: может привести к сложным и громоздким выражениям
- **Автоматическое дифференцирование**: использует правила дифференцирование на уровне вычислительного графа и основано на цепочном правиле дифференцирования. Например, если мы знаем, что в некоторой точке $x_0$ значение некоторой функции $f(x_0) = f_0$, а $f'(x_0) = f'_0$, тогда для функции $h(x) = f^2(x)$ мы можем найти и её значение и производную в точке $x_0: h(x_0) = f^2_0; h'(x) = 2f_0f'_0$. Это и есть цепочное правило дифференцирования.
