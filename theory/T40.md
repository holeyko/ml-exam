# Вариационный автокодировщик

Вариационные автокодировщики - это особый вид автокодировщиков, которые не просто учатся сжимать и разжимать данные, но и моделировать их вероятностное распределение. Это позволяет им генерировать новые данные, похожие на исходные, а также восстанавливать данные с большей точностью.

В отличие от обычного автокодировщика, кодировщик в VAE выдает не одну, а две вещи:

- среднее ($\mu$) распределения
- логарифм дисперсии ($\log{\sigma^2}$) распределения
Эти параметры определяют нормальное распределение, из которого будут генерироваться данные

**Боттлнек** - это центральный слой автокодировщика, который получает эти два параметра ($\mu$ и $\log{\sigma^2}$). Для создания сжатого представления данных, которое будет подаваться на вход декодировщику, используется техника репараметризации.

## Репараметризация

Репараметризация позволяет создать выборку $z$ из нормального распределения с параметрами $\mu$ и $\sigma$. Это делается следующим образом:

  $$z = \mu + \sigma \cdot \epsilon$$

Где $\epsilon$ - случайная величина, взятая из стандартного нормального распределения (среднее 0, дисперсия 1).

Цель - научиться генерировать новые данные, похожие на обучающие, за счёт моделирования латентного пространства как вероятностного распределения.

Понимаю, что всё вышесказанное звучит как бред, поэтому давайте сравним обычный автокодировщик с вариационным.

## Обычный автокодировщик

1. **Архитектура**:
    - Состоит из двух частей: энкодера и декодера.
    - Энкодер преобразует входные данные в компактное скрытое представление (код).
    - Декодер восстанавливает исходные данные из этого кода.
2. **Цель**:
    - Научиться сжимать данные в компактное представление и восстанавливать их обратно с минимальной потерей информации.
3. **Функция потерь**:
    - Основывается на разнице между исходными и восстановленными данными, обычно это среднеквадратичная ошибка (MSE).
4. **Скрытое представление**:
    - Конкретные значения скрытых переменных (код) для каждого примера.

## Вариационный автокодировщик (VAE)

1. **Архитектура**:
    - Также состоит из энкодера и декодера.
    - Вместо фиксированного кода энкодер VAE выводит параметры распределения (обычно среднее и дисперсию нормального распределения) скрытых переменных.
    - Код (латентное пространство) формируется путем выборки из этого распределения.
2. **Цель**:
    - Научиться генерировать новые данные, похожие на обучающие, за счет моделирования латентного пространства как вероятностного распределения.
3. **Функция потерь**:
    - Состоит из двух частей:
        1. **Reconstruction loss** (ошибка восстановления) - аналогична обычному автокодировщику и измеряет разницу между исходными и восстановленными данными.
        2. **Kullback-Leibler divergence (KL-дивергенция)** - мера различия между распределением, полученным энкодером, и стандартным нормальным распределением. Это помогает регуляризировать латентное пространство.
4. **Скрытое представление**:
    - Вероятностное представление скрытых переменных, которое позволяет взять обучающую выборку из распределения для декодера.

### Пример функции потерь VAE

Функция потерь VAE может быть выражена следующим образом:
$$
Loss=Reconstruction Loss+\beta \cdot KL Divergence
$$
где $\beta$ - весовой коэффициент, регулирующий вклад KL-дивергенции.

### Как это улучшает качество данных?

Использование параметров распределения и репараметризации помогает модели:

1. **Генерировать новые данные**: Благодаря тому, что модель обучается восстанавливать данные из распределения, она может создавать новые данные, следуя той же закономерности.
2. **Обеспечивать более устойчивое и плавное обучение**: Репараметризация помогает стабилизировать обучение и улучшить обобщающие способности модели.

## KL-дивергенция

KL дивергенция - это мера того, насколько одно вероятностное распределение отличается от другого.

### Пример

Предположим, у вас есть два мешка с шариками:

- В мешке A (P) 70% красных и 30% синих шариков.
- В мешке B (Q) 50% красных и 50% синих шариков.
KL дивергенция измеряет, насколько вероятности распределения A отличаются от распределения B.

### Формула KL дивергенции

Для двух вероятностных распределений P и Q, KL дивергенция вычисляется как:
$$
D_{KL}(P\|Q) = \sum_{x \in X}{P(x)log(\frac{P(x)}{Q(x)})}
$$
где:

- $P(x)$ и $Q(x)$ - вероятности события $x$ в распределениях $P$ и $Q$ соответственно.
- $X$ - множество всех возможных событий.

### Интуиция

1. **Если P и Q одинаковы**, то KL дивергенция будет равна 0. Это значит, что распределения идентичны.
2. **Если P и Q сильно различаются**, то KL дивергенция будет большой. Это значит, что распределения сильно отличаются.

В VAE KL дивергенция **помогает сделать распределение скрытых переменных более похожим на нормальное распределение**. Это упрощает выборку новых точек из латентного пространства и делает генерацию новых данных более осмысленной.

А ещё KL дивергенция **добавляет штраф**, если распределение скрытых переменных сильно отклоняется от нормального. Это предотвращает переобучение и способствует более устойчивому обучению модели.
