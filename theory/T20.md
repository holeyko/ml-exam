# SoftArgMax. SoftMax

**SoftArgMax** - это функция для вычисления вероятностного распределения наиболее вероятных позиций (аргументов) в заданном векторе.

Пусть $x$ - это выходные значения из алгоритма, причем $x$ - уверенность алгоритма в том, что объект принадлежит классу $i$, $x \in [-\infty, +\infty]$, найдем для них такие $p_1, \ldots, p_n$, что $p_i \in [0, 1]$ и $\sum_{i = 1}^{n}{p_{i}} = 1$, то есть $p_i$ - показывает, насколько вероятно, что каждая позиция будет самой вероятной:

$$
  p_{i} = \dfrac{\exp{(x_{i})}}{\sum_{j = 1}^{n}{\exp{(x_j)}}}
$$

Пусть $y = \text{SoftArgMax}{(x)}$, тогда:

$$
  \begin{aligned}
    \dfrac{\partial{y_{i}}}{\partial{x_{j}}} &=
    \begin{cases}
      y_{i} \cdot (1 - y_{j}), & i = j \\
      -y_{i} \cdot y_{j}, & i \neq j
    \end{cases} \\
    &= y_{i} \cdot (I[i = j] - y_{j})
  \end{aligned}
$$

Свойства $\text{SoftArgMax}$:

- Вычисляет по вектору чисел вектор с распределением вероятностей;
- Можно интерпретировать как вероятность нахождения максимума в $i$-й координате;
- $\text{SoftArgMax}{(x - c, y - c, z - c)} = \text{SoftArgMax}{(x, y, z)}$;
- Данная функция является частным случаем сигмоиды: $\sigma{(y)} = \text{SoftArgMax}{(y, 0)}$.

**SoftMax** - это функция, которая гладко аппроксимирует максимум: из *логарифмированного* мира попадаем в *обычный* с помощью *взятия экспоненты*, вычисляем сумму и попадаем обратно в логарифмированный мир взятием логарифма.

$$
  \text{SoftMax}{(x_1, \ldots, x_n)} = \log{\left(\sum_{i = 1}^{n}{(\exp{(x_{i})})}\right)}
$$

Свойства $\text{SoftMax}$:

- $\text{SoftMax}{(a, a, a,)} \neq a$;
- $\text{SoftMax}{(x + a, y + a, z + a)} = \text{SoftMax}{(x, y, z)} + a$;
- Производная этой функции - это $\text{SoftArgMax}$.

Отличие от $\sigma$: $\text{SoftMax}$ похож на функцию сигмоиды, но отличается тем, что нормирует выходные значения до вероятностного распределения, а сигмоида выдает только одно значение вероятности.
