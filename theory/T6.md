# Метод Adam

Алгоритм оптимизации, сочетающий идеи двух других методов оптимизации AdaGrad и RMSProp. Он адаптирует скорости обучения для каждого параметра, основываясь на первых и вторых моментах градиентов (по-сути среднее значение и дисперсия градиентов).

#### Основные шаги:
- Инициализация параметров
	- $\theta$ - параметры модели (веса)
	- $m_t = 0$ - первая моментная переменная (среднее значение градиентов)
	- $v_t = 0$ - вторая моментная переменная (среднеквадратичное значение градиентов)
	- $t = 0$ - счетчик шагов
- Гиперпараметры
	- $\alpha$ - скорость обучения (learning rate)
	- $\beta_1$ - коэффициент экспоненциального сглаживания для первой моментной переменной (обычно 0.9)
	- $\beta_2$ - коэффициент экспоненциального сглаживания для первой моментной переменной (0.999)
	- $\epsilon$ - небольшая константа для предотвращения деления на ноль
- Обновление моментов и параметров
	- На каждом шаге t алгоритм обновляет моменты и параметры следующим образом
		- Увеличивает счетчик шагов: $$t = t + 1$$
		- Вычисляем градиент функции потерь $g_t$ по параметрам $\theta$: $$g_t = \nabla_{\theta}f(\theta_t)$$
		- Обновляем первую моментную переменную (среднее значение градиентов): $$m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$$
		- Обновляем вторую моментную переменную (среднеквадратичное значение градиентов): $$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$$
		- Корректируем моменты для устранения смещения: $$\hat{m_t} = \frac{m_t}{1 - \beta^t_1}; \; \hat{v_t} = \frac{v_t}{1 - \beta^t_2}$$
		- Обновляем параметры модели: $$\theta_t = \theta_{t - 1} - \alpha \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}$$
#### Преимущества:
- Адаптивная скорость обучения
- Улучшает устойчивость
- Повышение скорости
