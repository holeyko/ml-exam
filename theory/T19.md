# Адаптивный и импульсный градиентный спуск

## Градиентный спуск

Пусть нам дано некоторое допустимое множество $X \subset \mathbb{R}^n$ и целевая функция $f : X \to \mathbb{R}$ и мы хотим решить задачу оптимизации, где критерием поиска может выступать минимум или максимум. С точки зрения гладкости $f$ мы берём классификацию методов **первого порядка**, то есть вычисления первых частных производных (к ним относится *градиентный спуск*).

**Градиентный спуск** решает задачу минимизации эмпирического риска:

- $\theta_{(0)}$ - некоторое начальное значение;
- $\theta_{(k + 1)} = \theta_{(k)} - \mu\nabla\mathcal{L}(\theta_{(k)})$ - здесь мы говорим, что наш новый вектор коэффициентов - это старый вектор коэффициентов минус градиент от функции ошибки в старой точке $\nabla\mathcal{L}(\theta_{(k)})$ помноженный на $\mu$ - шаг градиента (или: скорость сходимости). Функция градиента $\nabla$ показывает лишь направление наискорейшего убывания функции, то есть он показывает лишь направление - и дале по этому вектору мы шагаем с скоростью сходимостью.

### Адаптивный градиентный спуск

Оригинальный градиентный спуск выглядит так: $w^{(0)}$ - начальные значения, тогда

$$
  w^{(k + 1)} = w^{(k)} - \mu \cdot \dfrac{\partial{\mathcal{L}{(w^{(k)})}}}{\partial{w}}
$$

Идея **адаптивного градиентного спуска**: давайте как-то накапливать квадрат изменений и ими нормализовать ширину шага, то есть если мы вдоль одной координаты уже много раз проходили, то мы накопим большое изменение и вдоль этой координаты, после нормализации, будут изменения меньше, в ином случае - будем двигаться быстрее, так как не сильная нормализация будет.

Итак, пусть $w^{(0)}$ - начальные значения, тогда для каждого $i$ (координата):

$$
  \begin{aligned}
    g_{i, (k)} &= \dfrac{\partial{\mathcal{L}{\left(w^{(k)}\right)}}}{\partial{w_{i}}}, \\
    w_{i}^{(k + 1)} &= w_{i}^{(k)} - \dfrac{\mu}{\sqrt{G_{i, i}^{(k)} + \varepsilon}} \cdot g_{i, (k)},
  \end{aligned}
$$

где $G$ — диагональная матрица, где каждый диагональный элемент $\langle i, i \rangle$ — сумма квадратов градиентов $g_{i, (k)}$ до шага $k$, $\varepsilon$ — сглаживающая переменная, предотвращающая деление на ноль.

Особенности:

- Устраняет необходимость вручную настраивать скорость обучения, как это происходит с [оригиналом](#градиентный-спуск).
- Накопление квадратов градиентов в знаменателе приводит к тому, что в процессе обучения сумма продолжает расти, либо двигаться очень медленно.

### Импульсный градиентный спуск

Оригинальный градиентный спуск выглядит так: $w^{(0)}$ - начальные значения, тогда

$$
  w^{(k + 1)} = w^{(k)} - \mu \cdot \dfrac{\partial{\mathcal{L}{(w^{(k)})}}}{\partial{w}}
$$

Идея **импульсного градиентного спуска** в том, что мы будем накапливать *изменения* и изменять параметры через вектор изменений. То есть, наша точка движется с некой скоростью, вычисляя градиент мы изменяем эту скорость (по правилу сложению векторов), далее будет изменятся значение этой скорости - физическая аналогия.

Тогда, мы вычисляем градиент, при помощи него обновляем *вектор скорости* (скользящее среднее), вычисляем скорость и меняем параметр.

$$
  \begin{aligned}
    w^{(k + 1)} &= w^{(k)} - v^{(k)}, \\
    v^{(k + 1)} &= \gamma \cdot v^{(k)} + \mu \cdot \dfrac{\partial{\mathcal{L}{(w^{(k)})}}}{\partial{w}}
  \end{aligned}
$$

Особенности:

- В целом, быстрее на сложном "рельефе" при движении в правильном направлении.
- Всё ещё может пропускать минимумы.
