# ResNet. ReLU

## ResNet

TODO

## ReLU

Это функция активации, широко используемая в нейронных сетях из-за своей простоты и эффективности.
Функция преобразует входное значение $z$ следующим образом: $$ReLU(z) = max(0,z)$$
**Преимущества ReLU**:

- **Простота вычислений**
- **Решение проблемы исчезающих градиентов**: в отличии от сигмоидных и $tanh$ функций активации, ReLU не страдает от проблемы исчезающих градиентов
- **Спарсити**: ReLU приводит к обнулению выходов для отрицательных значений, что делает сеть более разреженной и эффективной
**Недостатки ReLU**:
- **Мертвые нейроны**: иногда нейроны могут "умирать", если их выход всегда равен нулю. Это может произойти, если веса обновляются таким образом, что выходные значения всегда отрицательны. **Решением** является использование модификации Leaky ReLU, допускающей небольшие отрицательные значения. $$Leaky ReLU(z) = \begin{cases} z, & \mbox{если } z > 0 \\ \alpha z, & \mbox{если } z \leq 0 \end{cases}$$
 \*обычно  $\alpha = 0.01$
