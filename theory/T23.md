# ResNet. ReLU

### ReLU

 **Искусственная нейронная сеть (ANN)**:   
- Искусственная нейронная сеть – это модель, которая состоит из множества нейронов, соединённых между собой.
- Каждый нейрон принимает входные данные, обрабатывает их и выдаёт выходное значение.
**Функция активации**:  
- Функция активации применяется к выходу нейрона, чтобы добавить нелинейность в модель.
- Нелинейность важна, потому что без неё нейронная сеть была бы просто линейной комбинацией входных данных, и она не могла бы моделировать сложные зависимости.

#### ReLU (Rectified Linear Unit)
- ReLU – это одна из наиболее популярных функций активации.
- Она проста и эффективна, что делает её широко используемой в нейронных сетях.

#### Как работает ReLU

**Формула ReLU**:
- ReLU вычисляется как $ReLU(x)=max⁡(0,x)$
- Это значит, что если входное значение меньше нуля, выход будет нулевым; если больше нуля, выход будет равен входу.

**Преимущества ReLU**:    
- Простота вычислений: ReLU легко и быстро вычисляется.
- Способствует разреженности: Многие значения становятся нулями, что может помочь с обобщением модели и уменьшением переобучения.
- Смягчение проблемы затухающих градиентов: ReLU помогает избегать проблемы, когда градиенты становятся слишком малыми, что замедляет обучение.

### Resnet

#### Основы ResNet

**ResNet (Residual Network)**:
- ResNet – это архитектура нейронной сети, предложенная Kaiming He и его коллегами в 2015 году.
- Основная идея ResNet заключается в использовании остаточных (residual) соединений, чтобы облегчить обучение очень глубоких сетей.

#### Проблема затухания градиентов

**Затухание градиентов**:
- В глубоких нейронных сетях, когда градиенты вычисляются в процессе обратного распространения ошибки (backpropagation), они могут становиться очень малыми по мере прохождения через слои.
- Это приводит к тому, что обновления весов становятся незначительными, и сеть обучается очень медленно или вообще не обучается.

#### Пропуск слоёв (skip connections)
- В ResNet используется концепция пропуска слоёв, где выход одного слоя передаётся напрямую через несколько слоёв, минуя их.
- Эти соединения называются "shortcut connections" или "skip connections".

#### Как работает ResNet

**Остаточный блок**:
- В обычной нейронной сети каждый слой пытается напрямую выучить целевую функцию $H(x)$.
- В ResNet каждый слой учит остаточную функцию $F(x)=H(x)−x$.
- Истинная функция представляется как $H(x)=F(x)+x$.
- Остаточные соединения добавляют входные данные слоя напрямую к выходным данным через несколько слоёв.

#### Почему пропуск слоёв помогает избежать проблемы затухающих градиентов
- Пропуск слоёв позволяет градиентам распространяться через сеть без уменьшения, так как они могут передаваться напрямую через shortcut connections.
- Это помогает сохранить информацию и градиенты, даже в очень глубоких сетях.

#### Соединения нейронов
- В ResNet нейроны могут быть соединены не только с нейронами предыдущего слоя, но и с нейронами через несколько слоёв.
- Это означает, что каждый нейрон получает информацию как от предыдущего слоя, так и от нескольких предыдущих слоёв.

#### Выбор количества слоёв для пропуска
- В ResNet обычно используются блоки, состоящие из двух или трёх слоёв с пропуском через один блок.
- Например, если блок состоит из двух слоёв, пропуск будет через один блок (два слоя), если из трёх слоёв – пропуск через три слоя.
- Это число выбирается на основе экспериментов и опыта, чтобы добиться лучшей производительности модели.
