# Гиперпараметры. Отличие от параметров

**Гиперпараметры** – это параметры в схеме обучения любого алгоритма предсказания. Сама по себе схема в общем случае выглядит следующим образом:

1. На вход алгоритма обучения модели $a$ мы подаём гиперпараметры, которые задают поведение алгоритма, и набор данных $X$.
2. $a$ возвращает некие параметры $\delta$.
3. Далее, на вход алгоритма предсказания $a_{\delta}$ мы подаём $\delta$ и объект $x$.
4. $a_{\delta}$ возвращает некое предсказание $y$.

Исходя из этого, **гиперпараметры** – это:

- параметры алгоритма обучения;
- параметры, которые не меняются во время обучения;
- параметры, которые можно установить до наблюдения набора данных;
- структурные параметры.

Задача настройки гиперпараметров – найти такие параметры $p_{\text{best}}$, при которых была бы минимальная ошибка предсказания, например, при совпадении классификаций.

Существует несколько основных методов поиска $p_{\text{best}}$:

- *Поиск по сетке* (*Grid search*).  По сути, перебор всевозможных значений гиперпараметров из полного набора параметров. Проблемы: слишком много времени, невозможно легко рассчитать время работы.
- *Случайный поиск* (*Random search*). Тот же поиск по сетке, но: на каждой итерации комбинация значений гиперпараметров выбирается случайно и число итераций обычно ограничивается лимитом вычислений. Преимущества: быстрее, больший шанс выбрать лучший набор значений, за счёт отсутствия статического "шага" параметров. Проблемы: есть вероятность не рассмотреть все категориальные переменные, также имеются сложности с выбором распределения при случайном выборе.
- *Оптимизация чёрного ящика* (*Black-Box Optimization*). Критерием остановки служат: достижение желаемого минимума значении функции ошибки и количество итераций. Оптимизация решает задачи с абстрактными и числовыми объектами. Операторами для работы оптимизации служат: *генерация* (создаёт новый объект), *мутация* (создаёт из $x$ новый объект $x'$, который похож на $x$) и кроссовер (создаёт из двух объектов третий, который одновременно похож на первый и второй).
- *Байесовская оптимизация*.

Они отличаются от параметров – внутренних параметров, получаемых автоматически в процессе обучения и не настраиваемых специалистами по данным.
