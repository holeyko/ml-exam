# Гиперпараметры. Отличие от параметров

**Гиперпараметры** – это параметры алгоритма обучения. Сама по себе схема в общем случае выглядит следующим образом:

1. На вход алгоритма обучения модели $a$ мы подаём гиперпараметры, которые задают поведение алгоритма, и набор данных $X$.
2. $a$ возвращает некие параметры $\delta$.
3. Далее, на вход алгоритма предсказания $a_{\delta}$ мы подаём $\delta$ и объект $x$.
4. $a_{\delta}$ возвращает некое предсказание $y$.

Исходя из этого, **гиперпараметры** – это:

- параметры алгоритма обучения;
- параметры, которые не меняются во время обучения;
- параметры, которые можно установить до наблюдения набора данных;
- структурные параметры.

Задача настройки гиперпараметров – найти такие параметры $p_{\text{best}}$, при которых была бы минимальная ошибка предсказания, например, при совпадении классификаций.

Они отличаются от параметров – внутренних параметров, получаемых автоматически в процессе обучения и не настраиваемых специалистами по данным.
