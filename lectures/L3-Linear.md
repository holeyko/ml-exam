# Линейные методы

- [Линейные методы](#линейные-методы)
  - [Нормализация](#нормализация)
    - [Декорреляция](#декорреляция)
      - [Гауссиан](#гауссиан)
      - [Сингулярное разложение (SVD)](#сингулярное-разложение-svd)
      - [Метод главных компонент (PCA)](#метод-главных-компонент-pca)
  - [Линейная регрессия](#линейная-регрессия)
    - [Гребневая регуляризация](#гребневая-регуляризация)

## Нормализация

При работе с любыми данными мы начинаем с их нормализации, чтобы в будущем иметь более лучшие модели на дальнейшее обученеи и предсказание.

### Декорреляция

Начнём с декорреляции. Предположим, что нам подаются данные уже *центрированные* (то есть те, которые прошли *центровку*), то есть часть нормализации уже сделали: $\mathbb{E}[X_j] = 0$.

Посчитаем *ковариационную матрицу*: $\Sigma(X) = \dfrac{1}{N}X^{T}X$. Теперь выделим из него корень ($\Sigma^{1/2}$ - *разложение Холецкого*), возьмем от неё обратную и умножим на матрицу $X$ мы получим декоррелированные данные - *декорреляция*.

$$
    \hat{X} = X \times \Sigma^{-1/2}(X)
$$

Весь процесс можно описать следующим образом: получив данные в разложении в виде *эллипса*, мы распределяем их в *окружность*.

![Декорреляция](assets/decorrelation.png)

Мы делаем нечто похожее на декорреляцию, когда вычисляем *расстояние Махаланобиса*, потому что там мы брали обратную матрицу от матрицы ковариации и умножали на него векторы. Это же формула используется и в вычислении плотности многомерного гауссиана:

$$
    f_{\mu, \Sigma}(x) = \dfrac{\exp{\left(-\dfrac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)}}{\sqrt{(2\pi)^k|\Sigma|}}
$$

#### Гауссиан

Гауссиан в многомерном пространстве выглядит так - её главная ось может не совпадать в осями координат - это соответствует тому, что матрица ковариации недиагональная.

![Гауссиан](assets/gaussian.png)

Поскольку мы умеем считать плотность, то мы умеем явно решать задачу классификации. Предположим, что у нас есть два множества, опишем их гауссианоми. Теперь для некоторой точки $x$ мы посчитаем плотность относительной первой гауссиана (для первого класса), посчитаем плотность относительно второго (для второго класса) - у кого больше плотность, того и классификация точки.

Также мы можем решать задачу генерации точек. Здесь нам потребуется вычисления корня и матрицы.

- $\Sigma = A^TA$ - матрица ковариации раскладывается на произведение двух матриц (одна из которых транспонированная);
- $z_i \gets \mathcal{N}(0, 1)$ - возьмем значение из нормального распределения (независимые координаты) по каждой из координат;
- $x \gets zA + \mu$ - генерация точки из произведения сгенерированного из нормально распределения на полученную матрицу и прибавляем вектор сдвига.

#### Сингулярное разложение (SVD)

Здесь также можно использовать так называемое *сингулярное разложение (SVD)*. Предположим, у нас есть некоторые данные. Также предположим, что деформировать мы их можем только по осям координат, что будет соответствовать тому, что мы умножаем на диагональную матрицу. Например, мы можем сделать его более "вытянутым" по оси $x$ (то есть, превратить в эллипс). А далее мы хотим повернуть его в какую-ту сторону с помощью трансформирующей матрицы. SVD выглядит так: $F = VDU^{\mathrm{T}}$, где $V$ отвечает за данные, $D$ - за деформацию, $U$ - матрица поворота.

**Теорема**. Любая матрица $F$ размера $n \times m$ может быть представлена в виде сингулярного разложения $F = VDU^T$, где

- $V = (v_1, ~ \ldots, ~ v_m)$ размера $n \times m$, являющаяся ортогональной: $V^TV = I_m$, столбцы $v_j$ - собственные вектора матрицы $FF^T$
- $D = \text{diag}{(\sqrt{\lambda_1}, ~ \ldots, ~ \sqrt{\lambda_m})}$ размера $m \times m$, $\sqrt{\lambda_j}$ - **сингулярные числа**, квадратные корни собственных значений $F^TF$
- $U = (u_1, ~ \ldots, ~ u_m)$ размера $m \times m$, являющаяся ортогональной: $U^TU = I_m$, столбцы $u_j$ - собственные вектора матрицы $F^TF$.

По другому мы можем представить себе следующим образом: представим какое-то скрытое пространство, в которое мы хотим проецировать данные. Тогда,

- $V$ представляет, как объекты соответствуют базисным векторам,
- $D$ представляет важность каждого базисного вектора,
- $U$ показывает, как признаки соответствуют базисным векторам.

#### Метод главных компонент (PCA)

Возьмем матрицу $X$ в SVD-разложении и её коварицонной матрицы, то есть

- $X = VDU^{\mathrm{T}}$ - сингулярное разложение $X$;
- $\Sigma = \dfrac{1}{n}X^{\mathrm{T}}X$ - ковариация;

Подставим одно в другое и получим:

$$
    \begin{aligned}
        \Sigma &= \dfrac{1}{n} X^{\mathrm{T}}X \\
        &= \dfrac{1}{n} U D^{\mathrm{T}} V^{\mathrm{T}} V D U^{\mathrm{T}} \\
        &= U \dfrac{D^{2}}{n} U^{\mathrm{T}}
    \end{aligned}
$$

Посмотрим на корень матрицы $\Sigma$: $\Sigma^{1/2} = U^{\mathrm{T}}\dfrac{D}{n}$ - полученная матрица поворачивает данные в *обратном направлении*. Заметим, что делая сингулярное разложение мы можем посмотреть, как будут располагаться наши данные вдоль каждой оси (то есть, вдоль каждого собственного вектора (то есть, столбцы $U_j$ и будут собственными векторами)) и исходя из этого понимать, как столбцы дальше нам брать.

## Линейная регрессия

Задача линейной регрессии состоит следующим образом. Имеется некоторая функция $f(x, \theta)$, которая зависит от $x$ и обучаемых параметров $\theta$ (то есть, функция предсказания), сама функция представляет из себя скалярное произведение (то есть, сумму обычных произведений по-компонентно),

$$
    f(x, \theta) = \sum_{j = 1}^{m}{\theta_jf_j(x)}, ~ \theta \in \mathbb{R}^{m}
$$

Так вот, получается, что если взять производную из функции ошибки и приравнять к нулю, то мы получим *некоторое* решение задачи аналитическим способом:

$$
    \begin{aligned}
        \dfrac{\partial{\mathcal{L}(\theta)}}{\partial{\theta}} &= 2F^{\mathrm{T}}(F\theta - y) = 0 \\
        \theta^{\star} &= (F^{\mathrm{T}}F)^{-1}F^{\mathrm{T}}y
    \end{aligned}
$$

Это так называемый **метод наименьших квадратов**. Обычно здесь вводят понятие **псевдообратной матрицы** (или: **обратное преобразование Мура-Пенроуза**): $F^{+} = (F^{\mathrm{T}}F)^{-1}F^{\mathrm{T}}$, тогда наше решение будет иметь вид $\theta^{\star} = F^{+}y$.

Матрица $P_F = FF^{+}$ - это, так называемая, **проекционная матрица**, тогда нашу функцию ошибок (или: минимальное приближение) мы можем представить следующим образом: $\mathcal{L}(\theta^{\star}) = \|P_Fy - y\|^2$.

### Гребневая регуляризация

А теперь посмотрим на регуляризацию. К задаче линейной регрессии метод наименьших квадратов можно модифицировать добавив квадрат наших коэффициентов, то есть:

$$
    \mathcal{L}_{\tau}(\theta) = \|F\theta - y\|^2 + \tau\|\theta\|^2
$$

Заметим, что если нам дана такая функция регуляризации, то решение в матричном виде модифицируется, однако, это всё ещё остаётся аналитическим решением, то есть:

$$
    \theta_\tau^{\star} = (F^{\mathrm{T}}F + \tau I_m)^{-1}F^{\mathrm{T}}y
$$

Заметим также, что нам здесь обязательно понадобится нормализованная матрица, потому что у нас есть разные коэффициенты при разных признаках, будет получаться так: если признака слишком большая дисперсия, то казалось бы, для него должен быть более маленький коэффициент регуляризации.
