# Деревья решений и композиция алгоритмов

- [Деревья решений и композиция алгоритмов](#деревья-решений-и-композиция-алгоритмов)
  - [Деревья решений](#деревья-решений)
    - [Выбор разделяющих правил](#выбор-разделяющих-правил)
    - [Выбор критерия ветвления](#выбор-критерия-ветвления)
    - [Выбор критерия остановки](#выбор-критерия-остановки)
    - [Выбор подрезки](#выбор-подрезки)
    - [Регрессионные деревья](#регрессионные-деревья)
    - [Анализ алгоритма дерева решений](#анализ-алгоритма-дерева-решений)
  - [Композиция алгоритмов](#композиция-алгоритмов)
  - [Синтез случайных алгоритмов](#синтез-случайных-алгоритмов)
    - [Случайный лес](#случайный-лес)
  - [Стэкинг](#стэкинг)
  - [Бустинг](#бустинг)
    - [AdaBoost](#adaboost)

## Деревья решений

**Дерево решений** — алгоритм классификации и регрессии. *Вершины* содержат разделяющие правила или вопросы, которые мы задаём к нашему датасету (зачастую вопросы обращены к признакам). *Ребро* — возможный ответ на вопрос в родительской вершине. Листья содержат решения (класс объекта для задачи классификации или число для задачи регрессии).

Важно, чтобы дерево было максимально компактным по высоте, так как тогда мы рискуем шанс получить переобученную модель.

Разберёмся с тем, как строить подобное дерево. Пусть у нас есть *множество разделяющих правил* $\mathcal{B}$ - это те самые вопросы, которые мы задаем к каждому объекту набора данных, - и есть определенный *критерий ветвления* $\Phi$ - предположим, что мы задали вопрос, разделил набор данных на несколько частей, и теперь мы хотим понять, насколько хороши мы поделили на две части. Итак, алгоритм:

1. Поместим весь наш набор данных $S$ в корень дерева.
2. На каждом шагу мы будем рекурсивно обрабатывать выборку $S$.
   1. Если у нас выборка содержит объекты только одного класса $c$, то создаём лист с классом $c$ и останавливаемся.
   2. Иначе, мы выбираем правило $b \in \mathcal{B}$, которое является наилучшим с точки зрения критерия $\Phi$, и разделяем выборку на $k$ частей: $S_1,\,\ldots,\,S_k$, где $k \geqslant 2$.
   3. Посмотрим на критерий остановки. Если он выполнился, то возвращаем наиболее популярный класс в текущей выборке $S$. В ином случае мы выдаём $k$ подвыборок $S_1,\,\ldots,\,S_k$ и далее каждого ребенка обкатываем рекурсивно.
3. После чего мы подрезаем итоговое дерево.

### Выбор разделяющих правил

Разделяющие правила работают с признаками: категориальные и числовыми. С категориальными можно только сравнивать - поэтому, мы сравнили на совпадение/несовпадение и приняли решение. С числовыми сложнее, так как пространство числовых значений непрерывно. Но обычно закрепляют некоторый признак и сравнивают - если больше некоторого значения, то в первую группу, иначе - в другую. Таким образом, в большинстве случаев выбираются правила для одного признака:

$$
  \begin{aligned}
    f_i(x) &> m_i \\
    f_i(x) &= d_i
  \end{aligned}
$$

Причем, можно создавать комбинацию таких правил. Существует великое множество способов разбить выборку. Все правила генерируются автоматически. Теперь мы хотим научиться разделять эти правила: тут вступает в игру $\Phi$ - критерий информативности, первый способ, - и помимо него мы можем попробовать без него вручную сделать следующее:

- либо объединить диапазоны значений,
- либо пропустить маленькие диапазоны.

Данным способом можно синтезировать правило для каждого признака.

А как же разбивать, наконец-таки? Разбивать можно на разное число частей ($k \geqslant 2$), всё зависит от правил(а). Если выборка каждый раз разделяется на две части, то есть при $k = 2$, то $\mathcal{B}$ - семейство бинарных правил, а дерево бинарное. Почему это вдруг хорошо? Хорошо это потому что мы можем тогда ускорять быстродействие дерева с помощью продвинутых алгоритмах на структура данных. Если признак категориальный, можно строить несколько рёбер из вершины, если численный - применяется бинаризация/дискретизация/корреляция и другое. Обычно, мы хотим чтобы на каждом шаге мы могли строить фиксированное количество ребёр $k$.

### Выбор критерия ветвления

Качество ветвления должно определять насколько хорошо мы разбили выборку. Для этого нужно вспомнить про *энтропию Шеннона*, или так называемая информационная энтропия. Энтропия по Шеннону для большого количества объектов вычисляется по формуле:

$$
  S = \dfrac{1}{N} \cdot \log_{2}{\dfrac{N!}{\prod_{i}{N_{i}!}}}
$$

От этой формулы мы избавляемся, приняв за веру, что $\log{N!} \approx N\log{N} - N$, получаем более простой вид:

$$
  \begin{aligned}
    S &= -\sum_{i}{\left(\dfrac{N_i}{N}\log_2{\dfrac{N_i}{N}}\right)} \\
    &= -\sum_{i}{p_i\log_2{p_i}}
  \end{aligned}
$$

Вы будем называть $\phi_h(S)$ - энтропией Шеннона, $\Phi_h(S)$ так называемый *IGain* или *информационный прирост*, считается он так: пусть $S_0$ - это посчитанная энтропия на всей ещё неразделенной выборке, тогда приростом будет служить:

$$
  \text{IGain} = S_0 - \sum_{i}{\dfrac{N_i}{N}S_i},
$$

то есть мы взяли вероятность встретить каждый $i$ класс, посмотрели на численность их внутри данной выборке (разделенной), просуммировали варианты и вычли - получили прирост информации. Грубо говоря, мы поделили на две новые выборки, и вычли их значения у $S_0$ - получили прирост.

Альтернативой в приросту может послужить так называемый *индекс Джини*: мы заменяем $\phi_g(x) = 1 - \sum_{i = 1}^{m}{p_i^2}$ в формуле *IGain* и получаем $\Phi_g(S)$ - *GiniGain*.

Ещё одной альтернативой может послужить *GainRatio*, где мы берем информационный прирост и делим его на энтропию родительской (неразделенной) выборки:

$$
  \text{GainRation}(S) = \dfrac{\text{IGain}(S)}{\text{Entropy}(S)}
$$

Показатель качества разделения обычно не влияет на эффективность дерева. Можно также учитывать веса объектов и веса признаков. Тогда,

- $\phi(S)$ зависит от вероятностей встречи класса $p_{c}$;
- $p_{c}$ можно вычислять, учитывая веса объектов $w(x)$, следующим образом - $p_{c}(S) = \dfrac{\sum_{x \in S}{w(x) \cdot [y(x) = c]}}{|S|_w}$;
- $|S|_w = \sum_{x \in S}{w(x)}$, также используется для $\Phi(S)$.

Фактически, мы в некоторых местах домножаем на вес объекта.

### Выбор критерия остановки

Самые известные и популярные критерии остановки:

- один из классов пустой после разбиения;
- если критерий информативности разделяющего правила $\Phi(S)$ ниже определённого порога, то можно также установить, что дальше нету смысла продолжать строить дерево;
- если количество элементов в вершине меньше некоторого порога, тогда тоже можно остановиться, например, при очень маленьких значениях - при трёх элементах;
- высота дерева выше определённого порога - от него зависит свойство переобучаемости нашего дерева.

Говоря о высоте дерева, мы можем также упомянуть ещё один критерий остановки. Фактически, он является предподрезкой дерева. Мы можем остановить рост дерева, когда не статистической значимости между признаком и классом в конкретном узле. Почему это важно? Потому что при построении дерева мы как раз всегда хотим иметь связь между целым объектом и некоторым классом - в случае о отсутствия корреляции между ними, следует остановиться. Обычно в таких случаях используется критерий $\chi^2$.

### Выбор подрезки

Зачем нам может понадобиться подрезка дерева? На это есть две основные причины:

1. Чем выше у нас располагаются вершины, чем ближе они к корню, тем сильнее они влияют на качество работы алгоритма.
2. Чем ниже у нас располагаются вершины, чем дальше они от корня, тем выше шанс на переобучение алгоритма.

Идея: давайте обрежем нижние ветки. По сути, мы обрабатываем созданные деревья при помощью совокупности **операторов упрощения**, например, по уменьшению количества ошибок. Как мы можем это сделать? Давайте сделаем так:

1. Давайте разобьём выборку в отношении $2/1$ на тренировочную и тестовую (валидационную).
2. Для каждой вершины применим некоторый определенный оператор упрощения, который является лучшим с точки зрения выбранного критерия качества (сначала построили дерево, потом оцениваем и что-то делаем с готовыми вершинами), мы можем:
   1. ничего не менять;
   2. заменить вершину "ребёнком" (для каждого "ребёнка"), то есть вырвать промежуточную вершину и подшить к ней ветку;
   3. или заменить вершину листом (для каждого класса).

Таким образом, дерево обрезается и становится короче.

### Регрессионные деревья

А теперь мы хотим решать задачу регрессии с помощью деревьев решений. Дерево в таком случае работает так же, только теперь возвращается среднее значение целевого признака вместо мажоритарного класса, то есть, теперь у нас меняется только критерий построения одного листа. И в качестве оценки построения $\phi$ можно использовать дисперсию (статистический критерий).

### Анализ алгоритма дерева решений

Преимущества:

- легко понять и интерпретировать;
- быстро обучается;
- может работать с разными типами данных (преимущественно лучше всех - с категориями);
- выполняет выбор признаков внутри себя.

Недостатки:

- чувствителен к шумам;
- быстро переобучается.

## Композиция алгоритмов

**Слабая обучаемость** - это та, когда за полиномиальное время можно найти алгоритм, производительность которого будет немного лучше чем 0.5.

**Сильная обучаемость** - это та, когда за полиномиальное время можно найти алгоритм, производительность которого будет сколь угодно высокой.

**Теорема**. *Сильная обучаемость эквивалентна слабой обучаемости, потому что любую модель можно усилить с помощью композиции алгоритмов.*

Сформулируем задачу на *композицию алгоритмов*. Предположим, что у нас есть множество объектов $X$, есть множество ответов $Y$, обучающая выборка $\mathcal{D} = \{(x_i,\,y_i)\}_{i = 1}^{|\mathcal{D}|}$ с известными ответами и семейство *базовых алгоритмов* $H$:

$$
  H = \{h(x, a) : X  \to R|a \in A\},
$$

где $a$ - вектор параметров, описывающий алгоритм, $R$ - некоторое подмножество (обычно - $\mathbb{R}$ или $\mathbb{R}^{M}$), $h(x, a)$ - некая вероятностная оценка. Наша задача - это найти (или: синтезировать) алгоритм, которы наиболее точно прогнозирует ответ для объектов из $X$.

Формально, **композиция** из $T$ базовых алгоритмов $h_1,\,\ldots,\,h_T : X \to R$ выражается как:

$$
  H_T(x) = C(F(h_1(x),\,\ldots,\,h_T(x))),
$$

где $C : R \to Y$ - *решающее правило*, а $F : R^T \to R$ - это *корректирующая функция*. Грубо говоря, $F$ должна создать совокупность всех базовых алгоритмов из всех данных, а $C$ на основе этого должна вынести вердикт.

Рассмотрим *решающее правило* $C(H(x)) \to Y$:

- Для задачи регрессии мы можем по простому взять результат совокупности и вернуть без изменений совокупность, либо же провести какие-нибудь изменения.
- Для задачи классификации на $k$ классах мы можем вернуть наиболее мажоритарный класс по результатам испытаний всех слабых моделей:

  $$
    C(F(h_1(x),\,\ldots,\,h_T(x))) = \argmax_{y \in Y}{h_y(x)}
  $$

- Для задачи бинарной классификации мы возвращаем знак: $C(H(x)) = \text{sign}(H(x))$.

Корректирующая функция, или функция, создающая совокупность, может быть представлена в виде простейшей модели *голосования* за некоторый класс: при классификации - это мажоритарное голосование (подсчёт "голосов"), а при регрессии - это мягкое голосование (подсчёт вероятностей, математическое ожидание и тому подобное). Также можно добавлять веса для участников голосования.

## Синтез случайных алгоритмов

Основная идея объединения алгоритмов: давайте строить различные алгоритмы, обучая модели при разных условиях. Более формально: будем обучать модели на различных "кусках данных". Как мы можем их обучать на разных "кусках данных"?

- **Subsampling**. Взять подмножество объектов и обучить.
- **Bagging**. Также берем подмножества объектов для каждого из алгоритмов, но одинакового размера с "бутстрапом" (случайный выбор с повторами).
- **Метод случайного подпространства**. Давайте будем использовать разные признаковые подпространства и на разных подпространствах признаков обучать наш набор данных.
- **Фильтрация**. Пусть у нас есть набор бесконечного размера. Обучим первый алгоритм на $X_1$, который содержит первые $m_1$ элементов. Далее бросаем монетку $m_2$ раз:
  - орёл: добавляем в $X_2$ первый некорректно классифицированный элемент;
  - решка: добавляем в $X_2$ первый корректно классифицированный элемент.

  Обучаем второй алгоритм на $X_2$ и делаем тоже самое, пока не получим несколько алгоритмов обученных на объектах, которые имеют схожие описания. Дальше, при ассемблировании, мы будем использовать метод голосования для получения вердикта на всех алгоритмах.

### Случайный лес

Пусть у нас есть набор данных $\mathcal{D} = \{x_i, y_i\}_{i = 1}^{|\mathcal{D}|}$ с $n$ признаками. Тогда,

1. Выбираем подмножество какого-то фиксированного размера с повторениями.
2. Дальше синтезируем дерево решений, то есть, фактически, строим деревья решений таким образом, что для каждого дерева выбиралось $\sqrt{n}$ случайных признаков. При этом, сами деревья могут быть самыми примитивными, подрезка не применяется.
3. Повторяем п.1 и п.2 много-много раз.

Далее, при предсказании мы используем метод голосования для вынесения вердикта. На самом деле, деревья очень легко переобучаются на разных подвыборках, но тут идея в том, что этих подвыборок много и они все маленькие - нам неважно, что деревья переобучаются на них. Заметим также, что по мере роста выборки его показатели качества сходятся (то есть, качество).

Как агрегировать?

- Опять же - голосование. Деревья возвращают классы. Среди них ищется чаще всего встречающийся класс.
- Деревья могут возвращать распределения вероятностей или весов классов, которые суммируются для каждого класса. И далее полученные суммы используются как финальное распределение весов классов.

## Стэкинг

Идея *стэкинга*: вместо того, чтобы комбинировать алгоритмы, будем использовать их прогнозы для обучения других моделей. Эту штуку можно обобщить на использование результатов классификации как новых характеристик объектов.

Грубо говоря, мы взяли данные $\mathcal{D}$ и применили к нему алгоритм $a_1$, получили какой-то $\hat{y_1}$. После этого, мы присоединили $\hat{y_1}$ к данным $\mathcal{D}$ (в техническом плане: как новый столбец) в качестве новой характеристики и на этом обучаем алгоритм $a_2$ - получили $\hat{y_2}$ и таким образом строим такой "pipeline" пока не достигнем хорошего качества.

## Бустинг

Идея задачи *бустинга*: давайте попробуем синтезировать алгоритм следующим образом, где у нас результат будет описываться как сумма базовых алгоритмов с некоторыми коэффициентами:

$$
  H_T(x) = \sum_{t = 1}^{T}{b_th(x,a_t)},
$$

где $b_t \in \mathbb{R}$ - коэффициенты, минимизирующие эмпирический риск. Сам по себе эмпирический риск выглядит так:

$$
  \mathcal{L}(X_T, \mathcal{D}) = \sum_{i = 1}^{|\mathcal{D}|}{\mathcal{L}(H_T(x_i), y_i)} \to \text{min},
$$

где $H_T$ - алгоритм (он же: ансамбль), мы считаем ошибки по каждому объекту с функцией ошибок $\mathcal{L}$.

Концептуально, бустинг напоминает игру в гольф, где мы итеративно делаем удар и попадаем мячиком куда-нибудь, всё ближе и ближе к, возвращаясь к задаче, минимуму функции. Причем, каждая следующая итерация будет отличаться от предыдущего.

Предположим, что мы умеем наращивать нашу композицию шаг за шагом, то есть выразим $H_t(x)$ следующим образом: $H_t(x) = H_{t - 1}(x) + b_th(x, a_t)$, то есть мы суммируем предыдущий с некоторым шагом вида $b_t$ коэффициентом помноженным на $h(x, a_t)$, то есть на каждому шаге нам нужно искать пару $(a_t,\,b_t)$.

С точки зрения минимизации эмпирического риска нужно инкрементально оценивать градиент функции ошибки, то есть нам нужно оценивать ошибку нашей композиции на каждой итерации. Пусть $\mathcal{L}^{(t)}$ - вектор ошибок на каждом объекте длины $|\mathcal{D}|$ и наша функция $\mathcal{L}^{(t)} = \sum_{i = 1}^{|\mathcal{D}|}{\mathcal{L}(H_t(x_i),\,y_i)}$. Давайте попробуем найти градиент этой функции, возьмем частную производную по $H_{t - 1}$ и получим:

$$
  \begin{aligned}
    \nabla{\mathcal{L}_{i}^{(t - 1)}} &= \dfrac{\partial{\mathcal{L}_{i}^{(t - 1)}}}{\partial{H_{t - 1}}}(x_i) \\
    &= \dfrac{\partial{\left(\sum_{i}^{|\mathcal{D}|}{\mathcal{L}(H_{t - 1},\,y_i)}\right)}}{\partial{H_{t - 1}}}(x_i) \\
    &= \dfrac{\partial{\mathcal{L}(H_{t - 1},\,y_i)}}{\partial{H_{t - 1}}}(x_i)
  \end{aligned}
$$

Таким образом, мы получаем $H_t(x) = H_{t - 1}(x) - b_t\nabla{\mathcal{L}^{(t - 1)}}$ - то есть, фактически, каждый последующий алгоритм в ансамбле должен уметь находить вектор ошибок - фактически, это *изменение вектора ошибок*.

Как искать $b_t$? Для него мы просто решаем задачу оптимизации, грубо говоря, давайте подберём такой $b_t$, при котором разница (ниже) будет минимальной для уменьшения эмпирического риска.

$$
  b_t = \argmin_{b}{\sum_{i = 1}^{|\mathcal{D}|}{\mathcal{L}(H_{t - 1}(x_i) - b\nabla\mathcal{L}^{(t - 1)},\,y_i)}}
$$

А как найти вектор-градиент $\nabla\mathcal{L}^{(t - 1)}$? Вообще говоря, это *не базовый алгоритм*, это вектор ошибок, таким образом, чтобы его найти, давайте просто обучим алгоритм $a_t$ искать вектор ошибок:

$$
  \begin{aligned}
    a_t &= \argmin_{a \in A}{\sum_{i = 1}^{|\mathcal{D}|}{\mathcal{L}(h(x_i,\,a),\,\nabla{\mathcal{L}^{(t - 1)}})}} \\
    &= \text{Learn}\left(\left\{x_i\right\}_{i = 1}^{|\mathcal{D}|},\,\left\{\nabla{\mathcal{L}_{i}^{(t - 1)}}\right\}_{i = 1}^{|\mathcal{D}|}\right)
  \end{aligned}
$$

Обобщённый алгоритм:

1. Есть набор данных $\mathcal{D}$, количество моделей $T$.
2. $H_0(x)$ - обучаем нашу модель.
3. На каждом шаге $T$:
   1. Вычисляем градиент $\nabla{\mathcal{L}^{(t - 1)}}$.
   2. Каждый $a_t$ стремится предсказать $\delta$ между правильным решением и совокупностью всех подправок на ошибки.
   3. Вычисляем $b_t$ - то, что позволяет уменьшить эмпирический риск, ищется, например, градиентными методами.
   4. Нашли новый $H_t(x)$.

Могут быть разные функции ошибки. Обычно мы хотим, чтобы $\mathcal{L}$ был кусочно-линейным:

$$
  \mathcal{L} = \sum_{i = 1}^{|\mathcal{D}|}{M} = \sum_{i = 1}^{|\mathcal{D}|}{\left[y_i \cdot \sum_{t = 1}^{N}{\alpha_{t}H_{t}(x_{i}) < 0}\right]},
$$

где $M$ - это отступ (margin).

### AdaBoost

Поскольку AdaBoost появился раньше обобщения на бустинг, то данный алгоритм является неким "скелетом" для любого бустинга и поддерживает работоспособность без градиентного спуска. Идея здесь такая же, как в бустинге, в построении линейного многообразия: $H_T(x) = \sum_{t = 1}^{T}{b_th(x,\,a_t)}$, поскольку, мы решаем задачу классификации, значит $\mathcal{L}(H(x),\,y) = \mathcal{L}(H(x)y)$. Здесь рассматривается специфичная функция потерь $E(M) = \exp(-M)$. Как мы будем действовать? Допустим, мы построили какой-нибудь разделяющий "пень" $a_j$, дальше

1. Для $a_j$ мы посчитали ошибку: $\mathcal{L}_{j} = \sum_{i = 1}^{|\mathcal{D}|}{w_i I_{ji}}$.
2. Поскольку мы смогли посчитать ошибку $\mathcal{L}_j$, значит, мы можем посчитать вес соответствующего классификатора: $b_{j} = \ln{\dfrac{1 - \mathcal{L}_{j}}{\mathcal{L}_{j} + 10^{-5}}}$.
3. После чего для каждого элемента выборки мы пересчитали его вес: $w_{i}^{j + 1} = \dfrac{w_i^j \cdot \exp{(-b_j I_{ji})}}{\sum_{i = 1}^{n}{w_i^{j + i}}}$.

Теперь возвращаемся к градиентному бустингу и подставим соответствующую функцию потерь $\mathcal{L}(H(x),\,y) = \mathcal{L}(H(x)y)$ в градиент:

$$
  \nabla{\mathcal{L}_{i}^{(t)}} = y_{i}w_{i},
$$

где $w_{i} = \dfrac{\partial{\mathcal{L}(H_{t - 1}y_i)}}{\partial{(H_{t - 1}y_i)}}(x_i)$ - весь объекта $x_i$.

Подводя итог, перескажем обобщённый алгоритм:

1. У нас есть набор данных $\mathcal{D}$ и количество алгоритмов $T$.
2. Для начала для каждого объекта мы устанавливаем в $\dfrac{1}{|\mathcal{D}|}$ - одинаковые веса.
3. Далее $T$ раз мы повторяем цикл.
   1. Считаем $a_t$ - строим разделяющий "пень".
   2. Считаем $N_t$ - функция ошибок, количество элементов, которые проиграли.
   3. Считаем $b_t$ - пересчитываем коэффициент соответствующим классификатором.
   4. Для каждого объекта из $\mathcal{D}$ мы пересчитываем вес: $w_i = w_i\exp{(-b_t \cdot y_i \cdot h(x_i, a))}$.
   5. Нормализуем веса (делим на сумму весов).

Подведём итоги. Преимущества:

- очень тяжело переобучить;
- можно применять для различных функций сложности.

Недостатки:

- нет обработки шумов;
- не может применяться для мощного алгоритма;
- трудно интерпретировать.
