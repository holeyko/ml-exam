# Автоматическое дифференцирование

- [Введение в глубокое обучение](#Введение-в-глубокое-обучение)
  - [Из чего состоит машинное обучение](#Из-чего-состоит-машинное-обучение)
  - [Выбор модели для данных](#выбор-модели-для-данных)
  - [Сведение задачи обучения к задачи оптимизаци](#сведение-задачи-обучения-к-задачи-оптимизаци)
  - [Ошибки и эмпирический риск](#ошибки-и-эмпирический-риск)
- [Автоматическое дифференциирование](#Автоматическое-дифференциирование)


## Введение в глубокое обучение

### Из чего состоит машинное обучение
Задача машинного обучения состоит из трёх частей: набор данных, модель, функция ошибки.

В машинном обучении мы пытаемся апроксимировать различные вещи из реального мира, 
используя данные и модели.

Пример:

В реальном мире это могут быть: Потенциально бесконечное множество объектов, 
закономерность или зависимость, бизнес метрика.

В машинном обучении это же соответственно: Набор данных (выборка), 
модель или функция, эмпирический риск или функция ошибки.

В глубоком обучении мы работаем с более сложными данными и функциями, 
но они должны быть дифференцируемыми для решения задачи оптимизации.

Глубокое обучение позволяет работать с нетабличными данными. 
Но функция, модель, ошибки -- сложнее. Также должны оставаться дифференцируемыми, 
т.к. задача оптимизации решается через дифференцирование.

### Выбор модели для данных

Модели с обучаемыми параметрами:
1. Полином(аппрксимация):
   Модель: $y(x) = a_0 + a_1 x + a_2 x ^ 2$... .
   Обучаемые параметры: $a_0, a_1, a_2$... .
2. Кривая второго порядка(задача классификации):
   Модель: $a_0 y^2 + a_1 x^2 + a_2 xy + a_3 y + a_4 x + a_5 > 0$.
   Обучаемые параметры: $a_0, a_1, a_2, a_3, a_4, a_5$.
3. Синусоида(периодическая):
   Модель: $y(x) = sin(a_0 x + a_1) \cdot a_2 + a_3$.
   Обучаемые параметры: $a_0, a_1, a_2, a_3$.
4. Неизвестная модель(берем "мощную" функцию -- подход глубокого обучения):
   Модель: $f(x, y, a) = (p_1, p_2, p_3)$.
   Обучаемые параметры: $a_0, a_1, a_2$ ... .

### Сведение задачи обучения к задачи оптимизаци

Имеем: функция от большого числа аргументов, 
которые можно поделить на две части -- объекты и параметры.
На стадии обучения меняем параметры. $->$ 
Функция ошибки зависит от параметров, которые мы меняем на стадии обучения.

На стадии предсказания мы подставляем параметры и получаем функцию, 
зависящую только от объекта на котором мы применяем.

### Ошибки и эмпирический риск

#### Эмпирический риск -- дифференциируемый.
Аналогично оптимизации.
Пример: 

Перекрестная энтропия: $H(p, q) = - \sum_{i = 1}^{k} q_i \cdot log p_i$, 
где $q$ -- реальное распределение вероятностей, а $p$ -- предсказанное.
Для классификации вырождается метод максимального правдоподобия, 
так как $q = (0, ..., 1, ..., 0)$.

#### Классификация 

С ней сложнее, т.к. работаем с категориями. 
Они дискретны и не всегда применимо дифференциирование.

Выход: подменяем на задачу с которой мы уже сможем работать

Перекрестная энтропия и методы максимального правдоподобия(как частный случай) 
используют дифференцируемые функции для оценки вероятностей и ошибок.

#### Матрица неточностей

В матрицу неточностей записываем вектор вероятностей.
Таким образом мы можем сделать, например, точность и F-меру дифференциируемыми.
(Аналогично с другими функциями, которые что-то вычисляют из матрицы неточности)

Проблемы такого подхода: Никто не гарантирует, что функция ошибки будет выпуклой.
Несмотря на это задачу оптимизации все-равно решают с помощью гадиентного спуска.
Так как получается бысро и эффективно.

Вывод: Задача машинного обучения сводится к дифференциированию.

## Автоматическое дифференциирование