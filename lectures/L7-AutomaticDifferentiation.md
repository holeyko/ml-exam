# Автоматическое дифференцирование

- [Введение в глубокое обучение](#Введение-в-глубокое-обучение)
  - [Из чего состоит машинное обучение](#Из-чего-состоит-машинное-обучение)
  - [Выбор модели для данных](#выбор-модели-для-данных)
  - [Сведение задачи обучения к задачи оптимизаци](#сведение-задачи-обучения-к-задачи-оптимизаци)
  - [Ошибки и эмпирический риск](#ошибки-и-эмпирический-риск)
- [Автоматическое дифференциирование](#Автоматическое-дифференциирование)
  - [Дифференцирование составных функций по графу вычислений](#Дифференцирование-составных-функций-по-графу-вычислений)


## Введение в глубокое обучение

### Из чего состоит машинное обучение
Задача машинного обучения состоит из трёх частей: набор данных, модель, функция ошибки.

В машинном обучении мы пытаемся апроксимировать различные вещи из реального мира, 
используя данные и модели.

Пример:

В реальном мире это могут быть: Потенциально бесконечное множество объектов, 
закономерность или зависимость, бизнес метрика.

В машинном обучении это же соответственно: Набор данных (выборка), модель или функция, 
эмпирический риск или функция ошибки.

В глубоком обучении мы работаем с более сложными данными и функциями, но они должны 
быть дифференцируемыми для решения задачи оптимизации.

Глубокое обучение позволяет работать с нетабличными данными. 
Но функция, модель, ошибки $-$ сложнее. Также должны оставаться дифференцируемыми, 
т.к. задача оптимизации решается через дифференцирование.

### Выбор модели для данных

Модели с обучаемыми параметрами:
1. Полином(аппроксимация):
   Модель: $y(x) = a_0 + a_1 x + a_2 x ^ 2$... .
   Обучаемые параметры: $a_0, a_1, a_2$... .
2. Кривая второго порядка(задача классификации):
   Модель: $a_0 y^2 + a_1 x^2 + a_2 xy + a_3 y + a_4 x + a_5 > 0$.
   Обучаемые параметры: $a_0, a_1, a_2, a_3, a_4, a_5$.
3. Синусоида(периодическая):
   Модель: $y(x) = sin(a_0 x + a_1) \cdot a_2 + a_3$.
   Обучаемые параметры: $a_0, a_1, a_2, a_3$.
4. Неизвестная модель(берем "мощную" функцию $-$ подход глубокого обучения):
   Модель: $f(x, y, a) = (p_1, p_2, p_3)$.
   Обучаемые параметры: $a_0, a_1, a_2$ ... .

### Сведение задачи обучения к задачи оптимизаци

Имеем: функция от большого числа аргументов, которые можно поделить на две части 
$-$ объекты и параметры. На стадии обучения меняем параметры. $\to$ Функция ошибки 
зависит от параметров, которые мы меняем на стадии обучения.

На стадии предсказания мы подставляем параметры и получаем функцию, зависящую только 
от объекта на котором мы применяем.

### Ошибки и эмпирический риск

#### Эмпирический риск $-$ дифференциируемый.
Аналогично оптимизации.
Пример: 

Перекрестная энтропия: $$H(p, q) = - \sum^{k}_{i = 1} q_i \cdot log{ p_i}$$, где $q$ $-$
реальное распределение вероятностей, а $p$ $-$ предсказанное. Для классификации 
вырождается метод максимального правдоподобия, так как $q = (0, ..., 1, ..., 0)$.

#### Классификация 

С ней сложнее, т.к. работаем с категориями. Они дискретны и не всегда применимо дифференциирование.

Выход: подменяем на задачу с которой мы уже сможем работать

Перекрестная энтропия и методы максимального правдоподобия(как частный случай) 
используют дифференцируемые функции для оценки вероятностей и ошибок.

#### Матрица неточностей

В матрицу неточностей записываем вектор вероятностей.
Таким образом мы можем сделать, например, точность и F-меру дифференциируемыми.
(Аналогично с другими функциями, которые что-то вычисляют из матрицы неточности)

Проблемы такого подхода: Никто не гарантирует, что функция ошибки будет выпуклой.
Несмотря на это задачу оптимизации все-равно решают с помощью градиентного спуска.
Так как получается бысро и эффективно.

Вывод: Задача машинного обучения сводится к дифференциированию.

## Автоматическое дифференциирование

### Дифференцирование составных функций по графу вычислений

*Составная функция* $-$ функция, которая состоит из элементарных функций, 
для которых мы умеем вычислять производную. Как правило, описывается графом вычислений.

*Сложная функция* $-$ функция, тип значения которой отличен от скаляра, 
может быть вектором, матрицей и т.д.(Функция сложных объектов)

Если функция имеет много параметров, то ее развертывание может быть экспоненциально большим.

#### $\to$ *Функции храним в виде графа вычислений.* (Граф направленный ацикличный)

Для хранения функции в виде графа вычислений используются вершины $-$ базовые операции и 
ребра, кодирующие использование результатов из других вершин.

*Статический граф* вычислений строится до вычисления функции. Можно заранее оптимизировать.

*Динамический граф* вычислений строится во время вычисления функции. Более гибкий.

#### Имеем два типа дифференцирования: прямое и обратное.

Диффренциируем именно параметры, не объекты.

В *прямом* дифференцировании вычисляются производные всех вершин по всем входам, 
что может быть неэффективно. $\to$ Данный метод не применяестя

В *обратном* дифференцировании вычисляется производная выходной вершины по всем вершинам
в обратном порядке, что требует меньше вычислений. 

Недостаток: надо помнить предыдущие вычисления. Если функции не скалярные, например, 
матричные, то в каждом узле графа будет храниться матрица $-$ это плохо. 

$\to$ Оптимизация: корневая декомпозиция если граф "линейный"$-$ сохраняем некоторые 
вычисления с определенным интервалом. При необходимости вычисления производной возьмем 
2 соседних значения: пересчитаем промежуточные значения и производные в этом интервале. 

#### Пересчет производной

Блок умеет вычислять свою функцию и пересчитывать производную через себя.

Пример: Для умножения используется блок, который умножает аргумент на производную по 
выходу и наоборот.

#### Совмещение блоков

Блоки можно совмещать, например, два блока подряд для вычисления производной.

Если функция зависит от другой функции, то для вычисления производной нужно 
просуммировать производные по всем использованиям.

#### Цепное правило

Вершины могут переиспользоаться.

Расширенное цепное правило:

$$
\dfrac{1}{2}= \sum_{i}\dfrac{\delta f}{delta g_i}
$$

Если вершина - это простая функция, то производная через неё пересчитывается по 
стандартному цепному правилу.

Если воспользовались вершиной несколько раз, то производную нужно просуммировать по 
всем использованиям. Это следует из обобщённого цепного правила. Для сложной функции 
суммироваться будут не скаляры, а векторы, матрицы и т.д.