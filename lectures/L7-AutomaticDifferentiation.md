# Автоматическое дифференцирование

- [Введение в глубокое обучение](#Введение-в-глубокое-обучение)
  - [Из чего состоит машинное обучение](#Из-чего-состоит-машинное-обучение)
  - [Выбор модели для данных](#выбор-модели-для-данных)
  - [Сведение задачи обучения к задачи оптимизаци](#сведение-задачи-обучения-к-задачи-оптимизаци)
  - [Ошибки и эмпирический риск](#ошибки-и-эмпирический-риск)
- [Автоматическое дифференциирование](#Автоматическое-дифференциирование)


## Введение в глубокое обучение

### Из чего состоит машинное обучение
Задача машинного обучения состоит из трёх частей: набор данных, модель, функция ошибки.

В машинном обучении мы пытаемся апроксимировать различные вещи из реального мира, 
используя данные и модели.

Пример:

В реальном мире это могут быть: Потенциально бесконечное множество объектов, 
закономерность или зависимость, бизнес метрика.

В машинном обучении это же соответственно: Набор данных (выборка), 
модель или функция, эмпирический риск или функция ошибки.

В глубоком обучении мы работаем с более сложными данными и функциями, 
но они должны быть дифференцируемыми для решения задачи оптимизации.

Глубокое обучение позволяет работать с нетабличными данными. 
Но функция, модель, ошибки -- сложнее. Также должны оставаться дифференцируемыми, 
т.к. задача оптимизации решается через дифференцирование.

### Выбор модели для данных

Модели с обучаемыми параметрами:
1. Полином(аппрксимация):
   Модель: $y(x) = a_0 + a_1 x + a_2 x ^ 2$... .
   Обучаемые параметры: $a_0, a_1, a_2$... .
2. Кривая второго порядка(задача классификации):
   Модель: $a_0 y^2 + a_1 x^2 + a_2 xy + a_3 y + a_4 x + a_5 > 0$.
   Обучаемые параметры: $a_0, a_1, a_2, a_3, a_4, a_5$.
3. Синусоида(периодическая):
   Модель: $y(x) = sin(a_0 x + a_1) * a_2 + a_3$.
   Обучаемые параметры: $a_0, a_1, a_2, a_3$.
4. Неизвестная модель(берем "мощную" функцию -- подход глубокого обучения):
   Модель: $f(x, y, a) = (p_1, p_2, p_3)$.
   Обучаемые параметры: $a_0, a_1, a_2$ ... .

### Сведение задачи обучения к задачи оптимизаци

Имеем: функция от большого числа аргументов, 
которые можно поделить на две части -- объекты и параметры.
На стадии обучения меняем параметры. => 
Функция ошибки зависит от параметров, которые мы меняем на стадии обучения.

!!!Возможно стоит написать про маленькую и большую функцию ошибки(t = 0:05:00)

На стадии предсказания мы подставляем параметры и получаем функцию, 
зависящую только от объекта на котором мы применяем.

### Ошибки и эмпирический риск


## Автоматическое дифференциирование