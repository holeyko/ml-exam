# Извлечение признаков

- [Извлечение признаков](#извлечение-признаков)
  - [Метод главных компонент](#метод-главных-компонент)
    - [Одномерный случай](#одномерный-случай)
  - [Автокодировщик](#автокодировщик)
  - [t-SNE](#t-sne)

## Метод главных компонент

Один из популярных методов извлечения признаков. Про разницу между извлечением и выбором [здесь](L11-FeatureSelection.md).

### Одномерный случай

Мы хотим, чтобы признак, который мы получаем, он был *один* и являлся линейной комбинацией. В качестве критерия качества или функции ошибки мы возьмем дисперсию проекций (должно быть максимальным) или расстояния от точек (должно быть минимальным) до неё.

Как это выглядит? Мы делаем проекцию. Если у нас новый признак - это линейная комбинация предыдущих, то сделав линейное преобразование, в данном случае, мы сделаем поворот и выкинем одну из координат и получим проекцию. В данном случае, у нас двумерное пространство. Также здесь у нас имеется честное расстояние проекции.

![Поворот](assets/turn.png)

Посмотрим на общий случай. Можно решать задачу следующим способом:

1. решить для меньшей размерности
2. искать проекцию с теми же критериями, но выбирать из новых осей ту, которая была бы перпендикулярной ко всем предыдущим

Говоря формально, для приближения данных линейным многообразием меньшего размера:

- минимизация расстояния
- максимизация дисперсии проекций
- максимизация расстояния между проекциями
- корреляция между осями проекций равна нулю

Эту задачу принято решать при помощи матричного метода, а именно используя сингулярное разложение (SVD). В качестве осей пространства принято считать собственные вектора, которые будут матрицей поворота нашего пространства. Также, мы хотим сортировать по собственным значениям.

> Здесь идёт какой-то пиздец с объяснением...
>
> **Теорема**. Если $k < \text{rank}(F)$, то минимум достигается, когда столбцы $U$ являются собственными столбцами $F^TF$, соответствующим $k$ максимальным собственным значениям, и $G = UF$.

Допустим, мы уже сделали проекцию. Нам нужно как-то выбрать число компонент. Если мы знаем число компонент - мы берём столько, например, при визуализации - возьмем собственный вектор с наибольшим собственным числом и все остальные по уменьшению величины. Если не знаем, то нам нужно как-то найти это количество - например, как в выборе признаков. Проблема подобна выбору $k$ в $EM$.

Далее, отсортируем собственные значения $F^TF$ по убыванию: $\lambda_{(1)} \geqslant \ldots \geqslant \lambda_{(n)}$. Тогда, охарактеризуем долю информации, теряемую при проекции $E(k)$:

$$
  E(k) = \dfrac{\|GU^T - F\|^2}{\|F\|^2} = \dfrac{\lambda_{(k + 1)} + \ldots + \lambda_{(n)}}{\lambda_{(1)} + \ldots + \lambda_{(n)}}
$$

В таком случае значение $k$ можно выбрать по $E(k)$.

Существуют разные модификации: можно, например, ядро (ядерная функция) добавить.

## Автокодировщик

**Автокодировщик** (**autoencoder**) - глубокая нейронная сеть, способная строить низкоразмерные представления данных за счет нелинейной трансформации.

Основная идея *автокодировщика*: давайте будем обучать две функции, одна из которых называется "кодировщик", а другая - "декодировщик".

**Кодировщик** (**encoder**) - часть сети от входного слоя до бутылочного горлышка. **Декодировщик** (**decoder**) - часть сети от бутылочного горлышка до выходного слоя. Выглядит это так:

![Кодировщик и декодировщик](assets/decoder-and-encoder.png)

Композиция эти функций должна быть *тождественной* функцией. То есть, мы обучаем из какого-то вектора $x$, получать тот же входной вектор $x$. В качестве подобных функций можно брать свёрточные функции, например, которые обрабатывают картинки.

Можно также по особому регуляризировать скрытое пространство: возьмем некоторую регуляризирущую функцию и наложим на неё ограничения на скрытое пространство, что вектор будет в нём, например, размером поменьше. Формально, вместо минимизации $\|d(c(x)) - x\|$ будем минимизировать

$$
  \|d(c(x)) - x\| + \tau \cdot L(c(x)),
$$

где $c$ - кодировщик, $d$ - декодировщик, $L$ - некая регуляризация, $\tau$ - коэффициент регуляризации. Стандартно можно взять $L_1$ норму (как в LASSO).

Классификации автокодировщиков:

- Шумоподавляющий (denoising) автокодировщик - вносим в наш вектор шум, а потом требуем вектор без шума.
- Сжимающий (contractive) автокодировщик.
- Вариационный (variational) автокодировщик.

## t-SNE

Есть ещё один метод построения скрытого представления пространства меньшего размерности - это так называемое **стохастическое вложение соседей с t-распределением**. Этот метод нелинеен, может быть использован в визуализации. Сильно отличается от PCIE тем, что он немного по другому оценивает решение задач, то есть до этого мы переводили наше пространство $X$ в $\hat{X}$, а потом обратно и оценивали, насколько они близки, теперь же - мы используем вероятности. Формально, наша идея выглядит следующим образом:

1. Определим вероятность для точки "выбрать ближайшим соседом" другую точку в пространстве.
2. Построим такие распределения для высокоразмерных и низкоразмерных представлений.
3. Минимизируем расстояние между двумя распределениями

В качестве функции ошибок используется обычно дивергенция. **Расстояние** (**дивергенция**) **Кульбака-Лейблера** (**KL divergence**) - расстояние между двумя распределениями $P$ и $Q$.

$$
  D_{\text{KL}}(P||Q) = \int_{-\infty}^{+\infty}{p(x)\log{\dfrac{p(x)}{q(x)}}\,\partial{x}},
$$

где $p$ распределено согласно $P$, а $q$ - согласно $Q$. Также называется *относительной энтропией*.
